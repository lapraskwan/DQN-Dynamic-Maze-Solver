{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Dynamic Maze with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "1. Better maze explore algorithm (wall following?) \n",
    "\n",
    "Warm up to get some signal from the goal state, and then use low epsilon q learning with HER (the furthest position as the additional goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2984,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "\n",
    "from read_maze import load_maze, get_local_maze_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_local_maze_information(x, y)` returns the information around location (y,x) (yes they are swapped...it should be `get_local_maze_information(row, col)`...) with shape (3,3,2), i.e. (row, col, info). info: [wall, fire_count]. If wall = 0, then there is a wall, otherwise that location is either empty or has a fire, which will last for fire_count rounds.\n",
    "\n",
    "`around` is ordered as `around[row][col]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2985,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the maze (This should only be called ONCE in the entire program)\n",
    "load_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2986,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Provides all functions to interact with the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, goal_x=199, goal_y=199, start_x=1, start_y=1, fire=True):\n",
    "        self.timestep = 0\n",
    "        self.maze_size = 201\n",
    "        self.goal_x = goal_x\n",
    "        self.goal_y = goal_y\n",
    "        self.maze = torch.zeros((self.maze_size, self.maze_size, 2))\n",
    "        self.x = start_x\n",
    "        self.y = start_y\n",
    "        self.num_action = 5\n",
    "        self.fire = fire\n",
    "\n",
    "        # Make initial observation\n",
    "        self.around = torch.tensor(get_local_maze_information(self.y, self.x))\n",
    "        if not self.fire:\n",
    "            self.around[:, :, 1] = 0\n",
    "        self.update_maze()\n",
    "\n",
    "    def update_maze(self):\n",
    "        \"\"\"Update the maze according to self.around and decrement fire\"\"\"\n",
    "        self.maze[:, :, 1] = torch.where(self.maze[:, :, 1] > 0, self.maze[:, :, 1] - 1.0, self.maze[:, :, 1])\n",
    "        self.maze[self.y-1:self.y+2, self.x-1:self.x+2] = self.around\n",
    "\n",
    "    def get_legal_actions(self):\n",
    "        \"\"\"\n",
    "        Return all legal actions from current state. \n",
    "        Illegal actions: Walk out of the maze, walk into the wall, walk into a fire.\n",
    "        \"\"\"\n",
    "        # Stay\n",
    "        legal_actions = [0]\n",
    "        # Left\n",
    "        if self.around[1][0][0] == 1 and self.around[1][0][1] == 0 and self.x - 1 >= 0 and self.x - 1 < self.maze_size and self.y >= 0 and self.y < self.maze_size:\n",
    "            legal_actions.append(1)\n",
    "        # Right\n",
    "        if self.around[1][2][0] == 1 and self.around[1][2][1] == 0 and self.x + 1 >= 0 and self.x + 1 < self.maze_size and self.y >= 0 and self.y < self.maze_size:\n",
    "            legal_actions.append(2)\n",
    "        # Up\n",
    "        if self.around[0][1][0] == 1 and self.around[0][1][1] == 0 and self.x >= 0 and self.x < self.maze_size and self.y - 1 >= 0 and self.y - 1 < self.maze_size:\n",
    "            legal_actions.append(3)\n",
    "        # Down\n",
    "        if self.around[2][1][0] == 1 and self.around[2][1][1] == 0 and self.x >= 0 and self.x < self.maze_size and self.y + 1 >= 0 and self.y + 1 < self.maze_size:\n",
    "            legal_actions.append(4)\n",
    "        return legal_actions\n",
    "\n",
    "    def get_next_position(self, action):\n",
    "        \"\"\"\n",
    "        Return next position if action is taken. \n",
    "        Note that this is not really taking an action, the environment would not change.\n",
    "        Also note that it does not care whether the action is legal or not.\n",
    "        \"\"\"\n",
    "        if action == 0:  # Stay\n",
    "            x = self.x\n",
    "            y = self.y\n",
    "        elif action == 1:  # Left\n",
    "            x = self.x - 1\n",
    "            y = self.y\n",
    "        elif action == 2:  # Right\n",
    "            x = self.x + 1\n",
    "            y = self.y\n",
    "        elif action == 3:  # Up\n",
    "            x = self.x\n",
    "            y = self.y - 1\n",
    "        elif action == 4:  # Down\n",
    "            x = self.x\n",
    "            y = self.y + 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Action: {action}\")\n",
    "        return x, y\n",
    "\n",
    "    def make_action(self, action):\n",
    "        \"\"\"\n",
    "        Take 1 of the following actions: stay, left, right, up, down. \n",
    "        Increment timestep.\n",
    "        Update environment states (x, y, around, maze).\n",
    "\n",
    "        Return a reward: 0 if game ends, -1 otherwise.\n",
    "        \"\"\"\n",
    "        reward = -1.0\n",
    "\n",
    "        if action not in self.get_legal_actions():  # If action is illegal, stay at current position and discount reward by 1\n",
    "            action = 0\n",
    "            reward -= 1\n",
    "\n",
    "        self.x, self.y = self.get_next_position(action)\n",
    "\n",
    "        # Update agent states\n",
    "        self.timestep += 1\n",
    "        self.around = torch.tensor(get_local_maze_information(self.y, self.x))\n",
    "        if not self.fire:\n",
    "            self.around[:, :, 1] = 0\n",
    "        self.update_maze()\n",
    "\n",
    "        if self.game_end():\n",
    "            return 0.0\n",
    "\n",
    "        return reward\n",
    "\n",
    "    def game_end(self):\n",
    "        \"\"\"Return True if agent reaches the bottom right corner\"\"\"\n",
    "        if self.x == self.goal_x and self.y == self.goal_y:  # 201 - 1 - wall\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def restart(self, x=1, y=1):\n",
    "        \"\"\"\n",
    "        Move the agent to the starting position.\n",
    "        Note that some fire might remain in the maze because we cannot call load_maze() again, \n",
    "        but they should be far away from the starting point so it does not really matter.\n",
    "        \"\"\"\n",
    "        self.timestep = 0\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "\n",
    "        # Make initial observation\n",
    "        self.around = torch.tensor(get_local_maze_information(self.y, self.x))\n",
    "        if not self.fire:\n",
    "            self.around[:, :, 1] = 0\n",
    "        self.update_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2987,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv2d_size_out(size, kernel_size=3, stride=1):\n",
    "#     return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "# conv_out_size = conv2d_size_out(conv2d_size_out(33))\n",
    "\n",
    "class FeatureNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network for mapping agent states to a feature vector.\n",
    "    Input: agent state\n",
    "    Output: feature vector\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        \"\"\"out_channel: number of output channels in the last convolutional layer before being flattened and returned\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, 16, 3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(16, out_channel, 3, padding=\"same\")\n",
    "\n",
    "    def forward(self, state):\n",
    "        feature = F.relu(self.conv1(state))\n",
    "        feature = F.relu(self.conv2(feature))\n",
    "        feature = feature.view(feature.shape[0], -1)\n",
    "        return feature\n",
    "\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to estimate the q-values.\n",
    "    Input: feature vector output from the feature network\n",
    "    Output: q values\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim, num_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_action)\n",
    "\n",
    "    def forward(self, feature):\n",
    "        output = F.relu(self.fc1(feature))\n",
    "        output = self.fc2(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2988,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Agent that learns the optimal path to solve the maze using Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, lr=1e-4, target_update=10, batch_size=128, input_size=9, buffer_size=torch.inf, device='cpu'):\n",
    "        # Environment and initial position\n",
    "        self.env = env\n",
    "        self.x = env.x\n",
    "        self.y = env.y\n",
    "\n",
    "        # For Exploration\n",
    "        self.visited_times = torch.zeros((self.env.maze_size, self.env.maze_size))  # Reset everytime after calling restart()\n",
    "        self.last_visited_timestep = torch.zeros((self.env.maze_size, self.env.maze_size))  # Do NOT reset after calling restart()\n",
    "\n",
    "        # Training set up\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update  # Timesteps between target network update\n",
    "        self.input_size = input_size  # Size of feature_net input\n",
    "        self.out_channel = 32\n",
    "        self.feature_dim = self.out_channel * self.input_size ** 2\n",
    "        self.lr = lr\n",
    "        self.device = device\n",
    "        self.replay_buffer = []  # list of tuples: (state, action, next_state, reward, done)\n",
    "        self.buffer_size = buffer_size\n",
    "\n",
    "        # Define networks\n",
    "        self.feature_net = FeatureNetwork(self.get_state(self.y, self.x).shape[0], self.out_channel).to(device)\n",
    "        self.feature_optimizer = optim.Adam(self.feature_net.parameters(), lr=self.lr)\n",
    "        self.q_net = QNetwork(feature_dim=self.feature_dim, num_action=self.env.num_action).to(device)\n",
    "        self.q_optimizer = optim.Adam(self.q_net.parameters(), lr=self.lr)\n",
    "\n",
    "        self.target_feature_net = FeatureNetwork(self.get_state(self.y, self.x).shape[0], self.out_channel).to(device)\n",
    "        self.target_feature_net.load_state_dict(self.feature_net.state_dict())\n",
    "        self.target_feature_net.eval()\n",
    "        self.target_q_net = QNetwork(feature_dim=self.feature_dim, num_action=self.env.num_action).to(device)\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_q_net.eval()\n",
    "\n",
    "    def load_model(self, path):\n",
    "        \"\"\"Load model state dict from file\"\"\"\n",
    "        print(f\"Loading models from {path}\")\n",
    "        model_info = torch.load(path, map_location=self.device)\n",
    "        self.feature_net.load_state_dict(model_info[\"feature_net\"])\n",
    "        self.feature_optimizer.load_state_dict(model_info[\"feature_optimizer\"])\n",
    "        self.q_net.load_state_dict(model_info[\"q_net\"])\n",
    "        self.q_optimizer.load_state_dict(model_info[\"q_optimizer\"])\n",
    "\n",
    "        self.target_feature_net.load_state_dict(self.feature_net.state_dict())\n",
    "        self.target_feature_net.eval()\n",
    "        self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_q_net.eval()\n",
    "    \n",
    "    def load_replay_buffer(self, path):\n",
    "        \"\"\"Load replay buffer to the agent. Multiple replay buffers can be loaded by calling this function multiple times with different paths.\"\"\"\n",
    "        # Load previoius replay buffer\n",
    "        print(f\"Loading replay buffer from: {path}\")\n",
    "        previous_replay_buffer = torch.load(path, map_location=self.device)\n",
    "        self.replay_buffer.extend(previous_replay_buffer[\"buffer\"])\n",
    "        if len(self.replay_buffer) > self.buffer_size:\n",
    "            self.replay_buffer = random.sample(self.replay_buffer, self.buffer_size)\n",
    "        print(\"Number of data in replay buffer:\", len(self.replay_buffer))\n",
    "\n",
    "    def save_maze_policy(self, path):\n",
    "        \"\"\"Save the current policy as an image\"\"\"\n",
    "        policy_image = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "        for y in range(self.env.maze_size):\n",
    "            for x in range(self.env.maze_size):\n",
    "                if self.env.maze[y, x, 0]:  # not a wall\n",
    "                    # Comput the best action\n",
    "                    state = self.get_state(y, x).to(self.device)\n",
    "                    action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                    best_action = torch.argmax(action_values)\n",
    "                    policy_image[y, x] = best_action\n",
    "\n",
    "        # Plot heatmap to show frequency of visiting each position\n",
    "        newcmap = ListedColormap(['black', 'blue', 'red', 'yellow', 'white'])\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        im = ax.imshow(policy_image, cmap=newcmap, vmin=0, vmax=4)\n",
    "        fig.colorbar(im)\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_visit_frequency(self, path):\n",
    "        # Save a heatmap showing frequency of visiting each position\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        im = ax.imshow(self.visited_times, cmap='gray')\n",
    "        fig.colorbar(im)\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_explored_maze(self, path):\n",
    "        \"\"\"Save the explored positions as an image\"\"\"\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow((self.visited_times > 0), cmap=\"gray\")\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "\n",
    "    def get_state(self, y, x):\n",
    "        \"\"\"\n",
    "        Return a tensor representing the current state \n",
    "        A state is a stack of 2D tensors with dimension (self.input_size, self.input_size) representing \n",
    "        the agent's position, walls, fires, y and x coordinates of the top left corner of the input.\n",
    "        \"\"\"\n",
    "        # Find the top left corner of the window, such that it is not out of the maze\n",
    "        x_left = max(0, x - int((self.input_size+1)/2))\n",
    "        x_left = min(x_left, self.env.maze_size - self.input_size)\n",
    "        y_top = max(0, y - int((self.input_size+1)/2))\n",
    "        y_top = min(y_top, self.env.maze_size - self.input_size)\n",
    "\n",
    "        state = torch.zeros((6, self.input_size, self.input_size))\n",
    "        state[0, y - y_top, x - x_left] = 1  # Current position\n",
    "        state[1] = self.env.maze[y_top:y_top+self.input_size, x_left:x_left+self.input_size, 0]  # Walls\n",
    "        state[2] = self.env.maze[y_top:y_top+self.input_size, x_left:x_left+self.input_size, 1]  # Fires\n",
    "        state[3] = torch.full((self.input_size, self.input_size), y_top)\n",
    "        state[4] = torch.full((self.input_size, self.input_size), x_left)\n",
    "\n",
    "        return state\n",
    "\n",
    "    def restart(self, x=1, y=1):\n",
    "        \"\"\"Restart the environment. Called everytime before starting a new episode.\"\"\"\n",
    "        self.env.restart(x=x, y=y)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.visited_times = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "\n",
    "    def get_action(self, state, mode=\"full_explore\", epsilon=0.8):\n",
    "        \"\"\"Given a state, return an action according to the policy.\"\"\"\n",
    "        state = state.to(self.device)\n",
    "\n",
    "        legal_actions = self.env.get_legal_actions()\n",
    "\n",
    "        if mode == \"full_explore\":\n",
    "            action_to_least_visited_neighbour = None\n",
    "            min_visited_times = torch.inf\n",
    "            for action in range(5):\n",
    "                next_x, next_y = self.env.get_next_position(action)\n",
    "                if self.visited_times[next_y, next_x] <= min_visited_times:\n",
    "                    # Legal action, record it\n",
    "                    if action in legal_actions:\n",
    "                        min_visited_times = self.visited_times[next_y, next_x]\n",
    "                        action_to_least_visited_neighbour = action\n",
    "                    # Illegal action but it is due to fire: wait there\n",
    "                    elif self.env.maze[next_y, next_x, 0]:\n",
    "                        action_to_least_visited_neighbour = 0\n",
    "                        min_visited_times = self.visited_times[next_y, next_x]\n",
    "\n",
    "            return action_to_least_visited_neighbour\n",
    "\n",
    "        elif mode == \"warm_up\":  # Go to a position closest to the goal most of the time, pick random action otherwise\n",
    "            action_closest_to_goal = None\n",
    "            max_visited_timestep = -torch.inf\n",
    "            for action in range(5):\n",
    "                if action in legal_actions:\n",
    "                    next_x, next_y = self.env.get_next_position(action)\n",
    "                    if self.last_visited_timestep[next_y, next_x] > max_visited_timestep:\n",
    "                        max_visited_timestep = self.last_visited_timestep[next_y, next_x]\n",
    "                        action_closest_to_goal = action\n",
    "            if random.random() < 0.5:\n",
    "                return random.randint(0, 4)\n",
    "            return action_closest_to_goal\n",
    "\n",
    "        elif mode == \"q_learning\":\n",
    "            # Epsilon Greedy\n",
    "            if random.random() < epsilon:\n",
    "                # Return random action\n",
    "                return random.randint(0, 4)\n",
    "            else:\n",
    "                # Return Best Action\n",
    "                action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                max_action = torch.argmax(action_values).item()\n",
    "                return max_action\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "    def update_network(self, gamma=0.99):\n",
    "        \"\"\"Update network weights\"\"\"\n",
    "        batch = random.sample(self.replay_buffer, min(len(self.replay_buffer), self.batch_size))\n",
    "        state_batch = torch.stack([data[0].to(self.device) for data in batch])\n",
    "        action_batch = torch.tensor([data[1] for data in batch]).to(self.device)\n",
    "        next_state_batch = torch.stack([data[2].to(self.device) for data in batch])\n",
    "        reward_batch = torch.tensor([data[3] for data in batch]).to(self.device)\n",
    "        done = torch.tensor([data[4] for data in batch]).to(self.device)\n",
    "\n",
    "        #----- Compute the loss of q values prediction -----#\n",
    "        # Prediction (Only update against the action that the agent took)\n",
    "        q_values_prediction = self.q_net(self.feature_net(state_batch))  # q value for all actions\n",
    "        q_values_prediction = q_values_prediction[torch.arange(q_values_prediction.shape[0]), action_batch]  # q value for the selected action\n",
    "\n",
    "        # Target (Next state's best q value)\n",
    "        next_q_values = self.target_q_net(self.target_feature_net(next_state_batch))\n",
    "        next_best_q_values, _ = torch.max(next_q_values, dim=1)\n",
    "        next_best_q_values = next_best_q_values.detach()  # Stop Gradient\n",
    "\n",
    "        q_criterion = nn.SmoothL1Loss()\n",
    "        q_loss = q_criterion(q_values_prediction,  reward_batch + gamma * next_best_q_values * (1 - done))  # Multiply by (1 - done) to set target value of goal state to 0\n",
    "\n",
    "        #----- Backprop all losses -----#\n",
    "        self.feature_optimizer.zero_grad()\n",
    "        self.q_optimizer.zero_grad()\n",
    "\n",
    "        loss = q_loss\n",
    "        loss.backward()\n",
    "\n",
    "        self.feature_optimizer.step()\n",
    "        self.q_optimizer.step()\n",
    "\n",
    "        return loss.cpu().item()\n",
    "\n",
    "    def run_episode(self, start_x=1, start_y=1, mode=\"q_learning\", epsilon=0.8, gamma=0.99, max_timestep=20000, update=True, save=True, display=None):\n",
    "        \"\"\"\n",
    "        Run a single episode.\n",
    "        If both \"full_explore\" and \"warm_up\" modes are called, they should have the same start_x and start_y.\n",
    "        \n",
    "        Parameters: \n",
    "            - start_x: x-coordinate of the starting position\n",
    "            - start_y: y-coordinate of the starting position\n",
    "            - mode: controls how to run the current episode. (\"full_explore\", \"warm_up\" or \"q_learning\")\n",
    "            - epsilon: parameter for epsilon-greedy\n",
    "            - gamma: discount factor\n",
    "            - max_timestep: timestep before terminating the episode\n",
    "            - buffer_size: maximum size of replay buffer\n",
    "            - update: update network weights during the episode if True.\n",
    "            - save: save trajectory to replay buffer if True\n",
    "            - display: timesteps between outputing each progress message\n",
    "        \n",
    "        Return:\n",
    "            - timestep: timestep used to complete the episode\n",
    "            - reward: total reward\n",
    "            - loss: a list of losses during every timestep\n",
    "            - done: True if the episode ends with agent reaching goal state, False otherwise.\n",
    "        \"\"\"\n",
    "        # Start from start state\n",
    "        self.restart(x=start_x, y=start_y)\n",
    "\n",
    "        total_reward = 0\n",
    "        losses = []\n",
    "\n",
    "        # For displaying the first timestep of an episode only\n",
    "        action = 0\n",
    "        reward = 0\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        while not self.env.game_end() and self.env.timestep < max_timestep:\n",
    "            # Display training log\n",
    "            if display is not None and self.env.timestep % display == 0:\n",
    "                # Print current state\n",
    "                print(f\"Timestep: {self.env.timestep} | Position: ({self.x},{self.y}) | Epsilon: {epsilon:.4f} | Last Action: {action} | Reward: {reward:4f} | Time Used: {time.perf_counter() - start_time:.2f}\")\n",
    "                state = self.get_state(self.y, self.x).to(self.device)\n",
    "                action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                print(\"Q-values: \", action_values.detach().cpu().numpy())\n",
    "\n",
    "            # Store current state info\n",
    "            previous_y, previous_x = self.y, self.x\n",
    "            no_fire_around = self.env.around[[0, 1, 1, 2], [1, 0, 2, 1], 1].sum().item() == 0\n",
    "\n",
    "            # Select action\n",
    "            state = self.get_state(self.y, self.x)\n",
    "            action = self.get_action(state, mode=mode)\n",
    "\n",
    "            # Make an action, update agent's position and exploration info\n",
    "            reward = self.env.make_action(action)\n",
    "            self.y, self.x = self.env.y, self.env.x\n",
    "            self.visited_times[self.y, self.x] += 1\n",
    "            if mode == \"full_explore\":\n",
    "                self.last_visited_timestep[self.y, self.x] = self.env.timestep\n",
    "            # Set timestep of unvisited position as the visited position - 1. The start state should have value 0.\n",
    "            if mode == \"warm_up\" and self.last_visited_timestep[self.y, self.x] == 0 and (self.y != start_x or self.x != start_y):\n",
    "                self.last_visited_timestep[self.y, self.x] = self.last_visited_timestep[previous_y, previous_x] - 1\n",
    "\n",
    "            # Reward Shaping\n",
    "            # # Penalize going to previous state unless the action is stay\n",
    "            # if trajectory and torch.equal(self.get_state(self.y, self.x)[0], trajectory[-1][0][0]) and action != 0:\n",
    "            #     reward -= 1\n",
    "\n",
    "            # If agent stays while there is no fire around, agent gets -0.5 reward. This is to solve the problem where the agent tends to stay to get more reward.\n",
    "            # For example, q value of staying is 1, but max q of all other next states are < 1. Both rewards are -1 because they are not goal state, so the agent prefers to stay.\n",
    "            if action == 0 and no_fire_around:\n",
    "                reward -= 0.5\n",
    "\n",
    "            total_reward += reward\n",
    "\n",
    "            # Get next state\n",
    "            next_state = self.get_state(self.y, self.x)\n",
    "\n",
    "            # Save to replay buffer\n",
    "            if save:\n",
    "                self.replay_buffer.append([state, action, next_state, reward, int(self.env.game_end())])\n",
    "\n",
    "                if len(self.replay_buffer) > self.buffer_size:\n",
    "                    self.replay_buffer = random.sample(self.replay_buffer, self.buffer_size)\n",
    "\n",
    "            # Update networks\n",
    "            if update:\n",
    "                loss = self.update_network(gamma=gamma)\n",
    "                losses.append(loss)\n",
    "\n",
    "                if self.env.timestep % self.target_update == 0:  # Update target network\n",
    "                    self.target_feature_net.load_state_dict(self.feature_net.state_dict())\n",
    "                    self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        # Make sure target nets are up to date after an episode\n",
    "        if update:\n",
    "            self.target_feature_net.load_state_dict(self.feature_net.state_dict())\n",
    "            self.target_q_net.load_state_dict(self.q_net.state_dict())\n",
    "\n",
    "        print(f\"Timestep: {self.env.timestep} | Position: ({self.x},{self.y}) | Epsilon: {epsilon:.4f} | Last Action: {action} | Reward: {reward:4f} | Time Used: {time.perf_counter() - start_time:.2f}\")\n",
    "\n",
    "        # Return total reward\n",
    "        return self.env.timestep, total_reward, losses, self.env.game_end()\n",
    "\n",
    "    def train(self, full_explore=1, warm_up=50, q_learning=5, epsilon=0.8, min_epsilon=0.1, epsilon_decay=1e-3, move_start_position=True, previous_training_history_path=None, previous_eval_path=None, previous_episode_path=None, starting_position_path=None):\n",
    "        \"\"\"\n",
    "        Training loop.\n",
    "\n",
    "        Paremeters:\n",
    "            - full_explore: number of times to run an episode in \"full_explore\" mode.\n",
    "            - warm_up: number of times to run an episode in \"warm_up\" mode.\n",
    "            - q_learning: number of times to run an episode in \"q_learning\" mode per starting points. (If move_start_position is True and there are 5 starting points, (5 * q_learning) episodes will be executed in q_learning mode.)\n",
    "            - epsilon: initial value of epsilon for epsilon greedy exploration.\n",
    "            - min_epsilon: minimum epsilon value.\n",
    "            - epsilon_decay: epsilon decay constant subtracted from epsilon after each q_learning episode.\n",
    "            - move_start_position: move the starting point further away from the goal during training.\n",
    "            - previous_training_history_path: file path storing previous training history.\n",
    "            - previous_replay_buffer_path: file path storing previous replay buffer.\n",
    "        \n",
    "        Return:\n",
    "            - timestep: list of timestep used in q_learning episodes\n",
    "            - reward: list of rewards from q_learning episodes\n",
    "            - loss: a list of losses from q_learning episodes\n",
    "        \"\"\"\n",
    "        timesteps = []\n",
    "        rewards = []\n",
    "        losses = []\n",
    "\n",
    "        eval_timesteps = []\n",
    "        eval_rewards = []\n",
    "        eval_done = []\n",
    "\n",
    "        # Load previous training history\n",
    "        if previous_training_history_path is not None:\n",
    "            print(f\"Loading previous training history from: {previous_training_history_path}\")\n",
    "            previous_training_history = torch.load(previous_training_history_path, map_location=self.device)\n",
    "            timesteps = previous_training_history[\"timesteps\"]\n",
    "            rewards = previous_training_history[\"rewards\"]\n",
    "            losses = previous_training_history[\"losses\"]\n",
    "\n",
    "        # Load previous evaluation history\n",
    "        if previous_eval_path is not None:\n",
    "            print(f\"Loading previous evaluation history from: {previous_eval_path}\")\n",
    "            previous_eval_history = torch.load(previous_eval_path, map_location=self.device)\n",
    "            eval_timesteps = previous_eval_history[\"timesteps\"]\n",
    "            eval_rewards = previous_eval_history[\"rewards\"]\n",
    "            eval_done = previous_eval_history[\"done\"]\n",
    "\n",
    "        #----- Exploration Phase -----#\n",
    "        for i in range(full_explore):  # To explore the maze, initialize self.last_visited_timestep, update the maze\n",
    "            st = time.perf_counter()\n",
    "            print(f\"Starting Full Explore Episode {i}\")\n",
    "            timestep, reward, loss, done = self.run_episode(mode=\"full_explore\", display=500, save=False, update=False)\n",
    "\n",
    "            # Save exploration path as images\n",
    "            self.save_visit_frequency(f\"maze/frequency/f_{i}.png\")\n",
    "            self.save_explored_maze(f\"maze/explored/f_{i}.png\")\n",
    "\n",
    "            print(\"Time used for this episode:\", time.perf_counter() - st)\n",
    "            print()\n",
    "\n",
    "        #----- Warm Up Phase -----#\n",
    "        print(f\"Starting {warm_up} warm-up episodes.\\n\")\n",
    "        for i in range(warm_up):\n",
    "            st = time.perf_counter()\n",
    "            print(f\"Starting Warm Up Episode {i}\")\n",
    "            timestep, reward, loss, done = self.run_episode(mode=\"warm_up\", display=500, save=True, update=False)\n",
    "\n",
    "            # Save exploration path as images\n",
    "            if i+1 % 10 == 0:\n",
    "                self.save_visit_frequency(f\"maze/frequency/w_{i}.png\")\n",
    "                self.save_explored_maze(f\"maze/explored/w_{i}.png\")\n",
    "\n",
    "            torch.save({\"buffer\": self.replay_buffer}, \"replay_buffer.pth\")\n",
    "\n",
    "            print(\"Time used for this episode:\", time.perf_counter() - st)\n",
    "            print()\n",
    "\n",
    "        #----- Q-Learning Phase -----#\n",
    "        # Initialize the starting positions (starting from positions closer to the goal)\n",
    "        starting_position = []\n",
    "        if starting_position_path is not None:\n",
    "            starting_position = torch.load(starting_position_path, map_location=self.device)[\"starting_position\"]\n",
    "        elif move_start_position:\n",
    "            for y in range(self.env.maze_size):\n",
    "                for x in range(self.env.maze_size):\n",
    "                    starting_position.append((y, x))\n",
    "            # Sort according to the distance to goal state, in ascending order (goal position is the first element)\n",
    "            starting_position.sort(key=lambda p: self.last_visited_timestep[p[0], p[1]], reverse=True)\n",
    "            for i, p in enumerate(starting_position):  # Loop until last visited time step is zero\n",
    "                if self.last_visited_timestep[p[0], p[1]] == 0:\n",
    "                    break\n",
    "            starting_position = starting_position[1:i]  # Ignore goal state and all unexplored positions\n",
    "            starting_position.append((1, 1))  # Add the starting point back because it has a 0 last visited timestep as well\n",
    "        else:\n",
    "            starting_position = [(1, 1)]\n",
    "\n",
    "        # Save remaining startining positions for resume training\n",
    "        torch.save({\"starting_position\": starting_position}, \"starting_position.pth\")\n",
    "\n",
    "        print(f\"Starting {len(starting_position) * q_learning} q-learning episodes. {'With' if move_start_position else 'Without'} moving start position.\\n\")\n",
    "\n",
    "        # Load previous episode index and epsilon\n",
    "        if previous_episode_path is not None:\n",
    "            previous_episode_info = torch.load(previous_episode_path)\n",
    "            previous_episode_index = previous_episode_info[\"episode\"]\n",
    "            epsilon = previous_episode_info[\"epsilon\"]\n",
    "            print(f\"Resuming training from episode {previous_episode_index}\\n\")\n",
    "\n",
    "        # Run q_learning episodes\n",
    "        for i, (y, x) in enumerate(starting_position):\n",
    "            if previous_episode_path is not None and (i+1)*q_learning <= previous_episode_index: # Skip previously trained episodes\n",
    "                continue\n",
    "            # max_timestep = 20000\n",
    "            max_timestep = 5 * (self.last_visited_timestep[self.env.goal_y, self.env.goal_x] - self.last_visited_timestep[y, x])\n",
    "\n",
    "            for j in range(q_learning):\n",
    "                if previous_episode_path is not None and i*q_learning+j <= previous_episode_index: # Skip previously trained episodes\n",
    "                    continue\n",
    "\n",
    "                if (i*q_learning+j+1) % 200 == 0:\n",
    "                    timestep, reward, loss, done = self.run_episode(mode=\"warm_up\", display=500, save=True, update=False)\n",
    "\n",
    "                st = time.perf_counter()\n",
    "                print(f\"Starting Q-Learning Episode {i*q_learning+j}. Starting point: ({x},{y})\")\n",
    "\n",
    "                timestep, reward, loss, done = self.run_episode(start_x=x, start_y=y, mode=\"q_learning\",\n",
    "                                                                epsilon=epsilon, display=500, max_timestep=max_timestep, update=True, save=True)\n",
    "\n",
    "                # Save training histories and replay buffer\n",
    "                timesteps.append(timestep)\n",
    "                rewards.append(reward)\n",
    "                losses.append(loss)\n",
    "\n",
    "                if (i*q_learning+j+1) % 20 == 0:\n",
    "                    torch.save({\"timesteps\": timesteps, \"rewards\": rewards, \"losses\": losses}, \"training_histories.pth\")\n",
    "\n",
    "                if (i*q_learning+j+1) % 100 == 0:\n",
    "                    torch.save({\"buffer\": self.replay_buffer}, \"replay_buffer.pth\")\n",
    "                \n",
    "                # Save episode index for resume training\n",
    "                torch.save({\"episode\": i*q_learning+j, \"epsilon\": epsilon}, \"previous_episode.pth\")\n",
    "\n",
    "                # Epsilon decay\n",
    "                if epsilon > min_epsilon:\n",
    "                    epsilon -= epsilon_decay\n",
    "\n",
    "                print(\"Time used for this episode: \", time.perf_counter() - st)\n",
    "                print()\n",
    "\n",
    "            # Save exploration path and current policy as images\n",
    "            self.save_visit_frequency(f\"maze/frequency/q_{i*q_learning+j+1}.png\")\n",
    "            self.save_explored_maze(f\"maze/explored/q_{i*q_learning+j+1}.png\")\n",
    "            self.save_maze_policy(f\"maze/policy/q_{i*q_learning+j+1}.png\")\n",
    "            \n",
    "            if (i+1)*q_learning % 100 == 0:\n",
    "                # Evaluate policy by acting almost greedily\n",
    "                print(f\"Running evaluation episode\")\n",
    "                timestep, reward, _, done = self.run_episode(mode=\"q_learning\", epsilon=0.05, display=500,\n",
    "                                                                max_timestep=6000, update=False, save=False)\n",
    "                eval_timesteps.append(timestep)\n",
    "                eval_rewards.append(reward)\n",
    "                eval_done.append(done)\n",
    "                torch.save({\"timesteps\": eval_timesteps, \"rewards\": eval_rewards, \"done\": eval_done}, \"eval_histories.pth\")\n",
    "                print()\n",
    "            \n",
    "            # Save model\n",
    "            torch.save({\n",
    "                \"feature_net\": self.feature_net.state_dict(),\n",
    "                \"q_net\": self.q_net.state_dict(),\n",
    "                \"feature_optimizer\": self.feature_optimizer.state_dict(),\n",
    "                \"q_optimizer\": self.q_optimizer.state_dict()\n",
    "            }, \"models/q_learning.pth\")\n",
    "\n",
    "        #----- Finished all training -----#\n",
    "        # Save model\n",
    "        torch.save({\n",
    "            \"feature_net\": self.feature_net.state_dict(),\n",
    "            \"q_net\": self.q_net.state_dict(),\n",
    "            \"feature_optimizer\": self.feature_optimizer.state_dict(),\n",
    "            \"q_optimizer\": self.q_optimizer.state_dict()\n",
    "        }, \"models/q_learning.pth\")\n",
    "\n",
    "        torch.save({\"timesteps\": timesteps, \"rewards\": rewards, \"losses\": losses}, \"training_histories.pth\")\n",
    "        torch.save({\"buffer\": self.replay_buffer}, \"replay_buffer.pth\")\n",
    "\n",
    "        # Evaluate policy by acting almost greedily\n",
    "        print(f\"Running evaluation episode\")\n",
    "        timestep, reward, _, done = self.run_episode(mode=\"q_learning\", epsilon=0.05, display=500,\n",
    "                                                        max_timestep=6000, update=False, save=False)\n",
    "        eval_timesteps.append(timestep)\n",
    "        eval_rewards.append(reward)\n",
    "        eval_done.append(done)\n",
    "        torch.save({\"timesteps\": eval_timesteps, \"rewards\": eval_rewards, \"done\": eval_done}, \"eval_histories.pth\")\n",
    "        print()\n",
    "\n",
    "        # Save exploration path and current policy as images\n",
    "        self.save_visit_frequency(f\"maze/frequency/q_final.png\")\n",
    "        self.save_explored_maze(f\"maze/explored/q_final.png\")\n",
    "        self.save_maze_policy(f\"maze/policy/q_final.png\")\n",
    "\n",
    "        return timesteps, rewards, losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2989,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading models from models/q_learning.pth\n",
      "Loading replay buffer from: replay_buffer.pth\n",
      "Number of data in replay buffer: 200000\n"
     ]
    }
   ],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "env = Environment(199, 199, fire=False)\n",
    "# env = Environment(28, 135, fire=False)\n",
    "# env = Environment(48, 31, fire=False)\n",
    "# env = Environment(23, 9, fire=False)\n",
    "# env = Environment(10, 3, fire=False)\n",
    "# env = Environment(1, 3, fire=False)\n",
    "\n",
    "agent = QLearningAgent(env, device=device, buffer_size=200000)\n",
    "agent.load_model(\"models/q_learning.pth\")\n",
    "agent.load_replay_buffer(\"replay_buffer.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2990,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading previous training history from: training_histories.pth\n",
      "Loading previous evaluation history from: eval_histories.pth\n",
      "Starting Full Explore Episode 0\n",
      "Timestep: 0 | Position: (1,1) | Epsilon: 0.8000 | Last Action: 0 | Reward: 0.000000 | Time Used: 0.00\n",
      "Q-values:  [-83.67712  -85.291954 -83.08162  -83.188576 -83.68232 ]\n",
      "Timestep: 500 | Position: (41,43) | Epsilon: 0.8000 | Last Action: 3 | Reward: -1.000000 | Time Used: 5.23\n",
      "Q-values:  [-85.69417 -86.353   -84.91803 -86.23202 -85.00956]\n",
      "Timestep: 1000 | Position: (49,101) | Epsilon: 0.8000 | Last Action: 4 | Reward: -1.000000 | Time Used: 10.24\n",
      "Q-values:  [-88.28797  -89.12695  -87.81057  -87.82892  -88.604965]\n",
      "Timestep: 1500 | Position: (15,147) | Epsilon: 0.8000 | Last Action: 4 | Reward: -1.000000 | Time Used: 15.57\n",
      "Q-values:  [-95.3535   -96.359665 -95.010635 -94.76377  -95.17276 ]\n",
      "Timestep: 2000 | Position: (37,133) | Epsilon: 0.8000 | Last Action: 2 | Reward: -1.000000 | Time Used: 28.19\n",
      "Q-values:  [-92.75097 -92.356   -92.45366 -92.73482 -92.89316]\n",
      "Timestep: 2500 | Position: (127,155) | Epsilon: 0.8000 | Last Action: 2 | Reward: -1.000000 | Time Used: 35.34\n",
      "Q-values:  [-91.8297   -91.47254  -92.2768   -91.258965 -91.88688 ]\n",
      "Timestep: 3000 | Position: (163,147) | Epsilon: 0.8000 | Last Action: 2 | Reward: -1.000000 | Time Used: 45.66\n",
      "Q-values:  [-88.77329  -88.288895 -88.258514 -89.26521  -89.11062 ]\n",
      "Timestep: 3500 | Position: (153,169) | Epsilon: 0.8000 | Last Action: 4 | Reward: -1.000000 | Time Used: 51.02\n",
      "Q-values:  [-93.03855  -92.538155 -93.538315 -92.49233  -92.505745]\n",
      "Timestep: 4000 | Position: (143,149) | Epsilon: 0.8000 | Last Action: 4 | Reward: -1.000000 | Time Used: 56.26\n",
      "Q-values:  [-87.8596   -88.5552   -87.47015  -87.361305 -88.19862 ]\n",
      "Timestep: 4500 | Position: (197,131) | Epsilon: 0.8000 | Last Action: 1 | Reward: -1.000000 | Time Used: 61.57\n",
      "Q-values:  [-94.88303  -94.88813  -94.24955  -94.91748  -94.438675]\n",
      "Timestep: 5000 | Position: (190,191) | Epsilon: 0.8000 | Last Action: 2 | Reward: -1.000000 | Time Used: 66.84\n",
      "Q-values:  [-96.50534  -95.98759  -96.045135 -96.93371  -96.85273 ]\n",
      "Timestep: 5025 | Position: (199,199) | Epsilon: 0.8000 | Last Action: 4 | Reward: 0.000000 | Time Used: 67.10\n",
      "Time used for this episode: 68.71526095899753\n",
      "\n",
      "Starting 0 warm-up episodes.\n",
      "\n",
      "Starting 20830 q-learning episodes. With moving start position.\n",
      "\n",
      "Resuming training from episode 499\n",
      "\n",
      "Running evaluation episode\n",
      "Timestep: 0 | Position: (1,1) | Epsilon: 0.0500 | Last Action: 0 | Reward: 0.000000 | Time Used: 0.00\n",
      "Q-values:  [-82.568054 -83.06695  -82.04646  -82.35712  -81.82774 ]\n",
      "Timestep: 500 | Position: (23,3) | Epsilon: 0.0500 | Last Action: 2 | Reward: -1.000000 | Time Used: 5.66\n",
      "Q-values:  [-75.176926 -74.40935  -75.96174  -74.83978  -75.14217 ]\n",
      "Timestep: 1000 | Position: (17,9) | Epsilon: 0.0500 | Last Action: 1 | Reward: -1.000000 | Time Used: 11.33\n",
      "Q-values:  [-77.44277  -77.854836 -76.70236  -77.24196  -76.75949 ]\n",
      "Timestep: 1500 | Position: (20,11) | Epsilon: 0.0500 | Last Action: 4 | Reward: -2.000000 | Time Used: 16.62\n",
      "Q-values:  [-76.021355 -75.2224   -75.50533  -76.28589  -76.640205]\n",
      "Timestep: 2000 | Position: (17,11) | Epsilon: 0.0500 | Last Action: 1 | Reward: -1.000000 | Time Used: 22.22\n",
      "Q-values:  [-77.80221  -77.82678  -77.00833  -77.05096  -78.129875]\n",
      "Timestep: 2500 | Position: (21,10) | Epsilon: 0.0500 | Last Action: 0 | Reward: -1.500000 | Time Used: 27.59\n",
      "Q-values:  [-75.37385 -76.02843 -75.93435 -75.10978 -74.76669]\n",
      "Timestep: 3000 | Position: (19,11) | Epsilon: 0.0500 | Last Action: 4 | Reward: -2.000000 | Time Used: 33.17\n",
      "Q-values:  [-76.586845 -76.02999  -75.85214  -76.674385 -76.999306]\n",
      "Timestep: 3500 | Position: (20,11) | Epsilon: 0.0500 | Last Action: 0 | Reward: -1.500000 | Time Used: 38.73\n",
      "Q-values:  [-76.021355 -75.2224   -75.50533  -76.28589  -76.640205]\n",
      "Timestep: 4000 | Position: (19,11) | Epsilon: 0.0500 | Last Action: 4 | Reward: -2.000000 | Time Used: 44.21\n",
      "Q-values:  [-76.586845 -76.02999  -75.85214  -76.674385 -76.999306]\n",
      "Timestep: 4500 | Position: (21,11) | Epsilon: 0.0500 | Last Action: 2 | Reward: -1.000000 | Time Used: 49.52\n",
      "Q-values:  [-75.50107 -74.6817  -76.0232  -74.89914 -76.05102]\n",
      "Timestep: 5000 | Position: (17,9) | Epsilon: 0.0500 | Last Action: 1 | Reward: -1.000000 | Time Used: 56.09\n",
      "Q-values:  [-77.44277  -77.854836 -76.70236  -77.24196  -76.75949 ]\n",
      "Timestep: 5500 | Position: (17,9) | Epsilon: 0.0500 | Last Action: 0 | Reward: -1.500000 | Time Used: 61.72\n",
      "Q-values:  [-77.44277  -77.854836 -76.70236  -77.24196  -76.75949 ]\n",
      "Timestep: 6000 | Position: (20,11) | Epsilon: 0.0500 | Last Action: 4 | Reward: -2.000000 | Time Used: 67.29\n",
      "\n",
      "Starting Q-Learning Episode 500. Starting point: (184,193)\n",
      "Timestep: 0 | Position: (184,193) | Epsilon: 0.7501 | Last Action: 0 | Reward: 0.000000 | Time Used: 0.00\n",
      "Q-values:  [-96.22883  -95.70347  -95.61277  -96.53354  -96.650185]\n",
      "Timestep: 500 | Position: (186,189) | Epsilon: 0.7501 | Last Action: 4 | Reward: -2.000000 | Time Used: 191.70\n",
      "Q-values:  [-97.32136 -96.75912 -96.71235 -97.74321 -97.96374]\n",
      "Timestep: 535 | Position: (185,189) | Epsilon: 0.7501 | Last Action: 3 | Reward: -1.000000 | Time Used: 207.34\n",
      "Time used for this episode:  207.35522266698536\n",
      "\n",
      "Starting Q-Learning Episode 501. Starting point: (184,193)\n",
      "Timestep: 0 | Position: (184,193) | Epsilon: 0.7500 | Last Action: 0 | Reward: 0.000000 | Time Used: 0.00\n",
      "Q-values:  [-98.08196  -97.58581  -97.35034  -98.3996   -98.637375]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2990-0b8c7c24bfd0>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# timesteps, rewards, losses = agent.train(full_explore=1, warm_up=50, q_learning=5, epsilon=0.8, min_epsilon=0.3, epsilon_decay=1e-4, move_start_position=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#  timesteps, rewards, losses = agent.train(full_explore=1, warm_up=0, q_learning=5, epsilon=0.8, min_epsilon=0.3, epsilon_decay=1e-4, move_start_position=True)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_explore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_learning\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmin_epsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.3\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon_decay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1e-4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmove_start_position\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_training_history_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'training_histories.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_eval_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'eval_histories.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstarting_position_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'starting_position.pth'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprevious_episode_path\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'previous_episode.pth'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2988-50e7b4f89a9f>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, full_explore, warm_up, q_learning, epsilon, min_epsilon, epsilon_decay, move_start_position, previous_training_history_path, previous_eval_path, previous_episode_path, starting_position_path)\u001b[0m\n\u001b[1;32m    428\u001b[0m                 \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starting Q-Learning Episode {i*q_learning+j}. Starting point: ({x},{y})\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    429\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 430\u001b[0;31m                 timestep, reward, loss, done = self.run_episode(start_x=x, start_y=y, mode=\"q_learning\",\n\u001b[0m\u001b[1;32m    431\u001b[0m                                                                 epsilon=epsilon, display=500, max_timestep=max_timestep, update=True, save=True)\n\u001b[1;32m    432\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2988-50e7b4f89a9f>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, start_x, start_y, mode, epsilon, gamma, max_timestep, update, save, display)\u001b[0m\n\u001b[1;32m    289\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    290\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 291\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msample\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreplay_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuffer_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    292\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    293\u001b[0m             \u001b[0;31m# Update networks\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.9/random.py\u001b[0m in \u001b[0;36msample\u001b[0;34m(self, population, k, counts)\u001b[0m\n\u001b[1;32m    457\u001b[0m             \u001b[0mpool\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpopulation\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    458\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 459\u001b[0;31m                 \u001b[0mj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrandbelow\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    460\u001b[0m                 \u001b[0mresult\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    461\u001b[0m                 \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mj\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpool\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mn\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mi\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;31m# move non-selected item into vacancy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.9/random.py\u001b[0m in \u001b[0;36m_randbelow_with_getrandbits\u001b[0;34m(self, n)\u001b[0m\n\u001b[1;32m    241\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    242\u001b[0m         \u001b[0mgetrandbits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetrandbits\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 243\u001b[0;31m         \u001b[0mk\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbit_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# don't use (n-1) here because n can be 1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    244\u001b[0m         \u001b[0mr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgetrandbits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mk\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# 0 <= r < 2**k\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    245\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mr\u001b[0m \u001b[0;34m>=\u001b[0m \u001b[0mn\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    " # timesteps, rewards, losses = agent.train(full_explore=1, warm_up=50, q_learning=5, epsilon=0.8, min_epsilon=0.3, epsilon_decay=1e-4, move_start_position=True)\n",
    "#  timesteps, rewards, losses = agent.train(full_explore=1, warm_up=0, q_learning=5, epsilon=0.8, min_epsilon=0.3, epsilon_decay=1e-4, move_start_position=True)\n",
    "timesteps, rewards, losses = agent.train(full_explore=1, warm_up=0, q_learning=5, epsilon=0.8, min_epsilon=0.3, epsilon_decay=1e-4, move_start_position=True, previous_training_history_path='training_histories.pth', previous_eval_path='eval_histories.pth', starting_position_path='starting_position.pth', previous_episode_path='previous_episode.pth')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_history = torch.load('training_histories.pth')\n",
    "timesteps = training_history[\"timesteps\"]\n",
    "rewards = training_history[\"rewards\"]\n",
    "losses = training_history[\"losses\"]\n",
    "\n",
    "eval_history = torch.load('eval_histories.pth')\n",
    "eval_timesteps = eval_history[\"timesteps\"]\n",
    "eval_rewards = eval_history[\"rewards\"]\n",
    "eval_done = eval_history[\"done\"]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "timesteps = np.array(timesteps)\n",
    "average_timesteps = [timesteps[max(0, i-10):i+11].mean() for i in range(len(timesteps))]\n",
    "ax.plot(average_timesteps)\n",
    "ax.set_ylabel(\"Time steps\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Training Timesteps\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "rewards = np.array(rewards)\n",
    "average_rewards = [rewards[max(0, i-10):i+11].mean() for i in range(len(rewards))]\n",
    "ax.plot(average_rewards)\n",
    "ax.set_ylabel(\"Rewards\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Training Rewards\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "flattened_loss = [loss for episode_loss in losses for loss in episode_loss]\n",
    "flattened_loss = np.array(flattened_loss)\n",
    "average_loss = [flattened_loss[max(0, i-250):i+251].mean() for i in range(len(flattened_loss))]\n",
    "ax.plot(average_loss)\n",
    "ax.set_ylabel(\"Loss\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Training Loss\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scale the x_axis because the policy is only evaluated after running all episodes for each starting point\n",
    "x_axis = np.arange(len(eval_timesteps)) * 5\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "eval_timesteps = np.array(eval_timesteps)\n",
    "average_eval_timesteps = [eval_timesteps[max(0, i-10):i+11].mean() for i in range(len(eval_timesteps))]\n",
    "ax.plot(x_axis, average_eval_timesteps)\n",
    "ax.set_ylabel(\"Time steps\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Evaluation Timesteps\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "eval_rewards = np.array(eval_rewards)\n",
    "average_eval_rewards = [eval_rewards[max(0, i-10):i+11].mean() for i in range(len(eval_rewards))]\n",
    "ax.plot(x_axis, average_eval_rewards)\n",
    "ax.set_ylabel(\"Rewards\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Evaluation Rewards\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "eval_done = np.array(eval_done)\n",
    "average_eval_done = [eval_done[max(0, i-10):i+11].mean() for i in range(len(eval_done))]\n",
    "ax.plot(x_axis, average_eval_done)\n",
    "ax.set_ylabel(\"Reaching goal state\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "ax.set_title(\"Evaluation reaches goal state\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f414fa5d864e6fd9d435e1cda9914dfbcdafa27af1ac4ac359054dbf9afa5fd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
