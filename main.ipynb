{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Solving Dynamic Maze with Q-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO:  \n",
    "1. whole maze as cnn input;  \n",
    "2. cut it to grids and put it into different channels;  \n",
    "3. instead of warm up, use pure e-greedy q-learning;\n",
    "4. More self supervised?\n",
    "5. Add the prior to the target q value\n",
    "6. add explore new state terms to reward during warm up phase  \n",
    "\n",
    "Warm up to get some signal from the goal state, and then use low epsilon q learning with HER (the furthest position as the additional goal)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2880,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "import random\n",
    "import time\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.colors import ListedColormap\n",
    "# %matplotlib inline\n",
    "\n",
    "from read_maze import load_maze, get_local_maze_information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`get_local_maze_information(x, y)` returns the information around location (y,x) (yes they are swapped...it should be `get_local_maze_information(row, col)`...) with shape (3,3,2), i.e. (row, col, info). info: [wall, fire_count]. If wall = 0, then there is a wall, otherwise that location is either empty or has a fire, which will last for fire_count rounds.\n",
    "\n",
    "`around` is ordered as `around[row][col]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2881,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the maze (This should only be called ONCE in the entire program)\n",
    "load_maze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2883,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Environment:\n",
    "    \"\"\"\n",
    "    Provides all functions to interact with the environment\n",
    "    \"\"\"\n",
    "    def __init__(self, start_x=1, start_y=1, goal_x=199, goal_y=199):\n",
    "        self.timestep = 0\n",
    "        self.maze_size = 201\n",
    "        self.goal_x = goal_x\n",
    "        self.goal_y = goal_y\n",
    "        self.maze = torch.zeros((self.maze_size, self.maze_size, 2))\n",
    "        self.x = start_x\n",
    "        self.y = start_y\n",
    "        self.num_action = 5\n",
    "        # Make initial observation\n",
    "        self.around = torch.tensor(get_local_maze_information(self.y, self.x))\n",
    "        self.update_maze()\n",
    "    \n",
    "    def update_maze(self):\n",
    "        \"\"\"Update the maze according to self.around and decrement fire\"\"\"\n",
    "        self.maze[:, :, 1] = torch.where(self.maze[:, :, 1] > 0, self.maze[:, :, 1] - 1.0, self.maze[:, :, 1])\n",
    "        self.maze[self.y-1:self.y+2, self.x-1:self.x+2] = self.around\n",
    "     \n",
    "    def get_legal_actions(self):\n",
    "        \"\"\"\n",
    "        Return all legal actions from current state. \n",
    "        Illegal actions: Walk out of the maze, walk into the wall, walk into a fire.\n",
    "        \"\"\"\n",
    "        # Stay\n",
    "        legal_actions = [0]\n",
    "        # # Left\n",
    "        # if self.around[1][0][0] == 1 and self.x - 1 >= 0 and self.x - 1 < self.maze_size and self.y >= 0 and self.y < self.maze_size:\n",
    "        #     legal_actions.append(1)\n",
    "        # # Right\n",
    "        # if self.around[1][2][0] == 1 and self.x + 1 >= 0 and self.x + 1 < self.maze_size and self.y >= 0 and self.y < self.maze_size:\n",
    "        #     legal_actions.append(2)\n",
    "        # # Up\n",
    "        # if self.around[0][1][0] == 1 and self.x >= 0 and self.x < self.maze_size and self.y - 1 >= 0 and self.y - 1 < self.maze_size:\n",
    "        #     legal_actions.append(3)\n",
    "        # # Down\n",
    "        # if self.around[2][1][0] == 1 and self.x >= 0 and self.x < self.maze_size and self.y + 1 >= 0 and self.y + 1 < self.maze_size:\n",
    "        #     legal_actions.append(4)\n",
    "        # Left\n",
    "        if self.around[1][0][0] == 1 and self.around[1][0][1] == 0 and self.x - 1 >= 0 and self.x - 1 < self.maze_size and self.y >= 0 and self.y < self.maze_size:\n",
    "            legal_actions.append(1)\n",
    "        # Right\n",
    "        if self.around[1][2][0] == 1 and self.around[1][2][1] == 0 and self.x + 1 >= 0 and self.x + 1 < self.maze_size and self.y >= 0 and self.y < self.maze_size:\n",
    "            legal_actions.append(2)\n",
    "        # Up\n",
    "        if self.around[0][1][0] == 1 and self.around[0][1][1] == 0 and self.x >= 0 and self.x < self.maze_size and self.y - 1 >= 0 and self.y - 1 < self.maze_size:\n",
    "            legal_actions.append(3)\n",
    "        # Down\n",
    "        if self.around[2][1][0] == 1 and self.around[2][1][1] == 0 and self.x >= 0 and self.x < self.maze_size and self.y + 1 >= 0 and self.y + 1 < self.maze_size:\n",
    "            legal_actions.append(4)\n",
    "        return legal_actions\n",
    "    \n",
    "    def get_next_position(self, action):\n",
    "        \"\"\"\n",
    "        Return next position if action is taken. \n",
    "        Note that this is not really taking an action, the environment would not change.\n",
    "        Also note that it does not care whether the action is legal or not.\n",
    "        \"\"\"\n",
    "        if action == 0:  # Stay\n",
    "            x = self.x\n",
    "            y = self.y\n",
    "        elif action == 1:  # Left\n",
    "            x = self.x - 1\n",
    "            y = self.y\n",
    "        elif action == 2:  # Right\n",
    "            x = self.x + 1\n",
    "            y = self.y\n",
    "        elif action == 3:  # Up\n",
    "            x = self.x\n",
    "            y = self.y - 1\n",
    "        elif action == 4:  # Down\n",
    "            x = self.x\n",
    "            y = self.y + 1\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown Action: {action}\")\n",
    "        return x, y\n",
    "            \n",
    "    def make_action(self, action):\n",
    "        \"\"\"\n",
    "        Take 1 of the following actions: stay, left, right, up, down. \n",
    "        Increment timestep.\n",
    "        Update agent states (x, y, around, maze).\n",
    "\n",
    "        Return a reward: 0 if game ends, -1 otherwise.\n",
    "        \"\"\"\n",
    "        reward = -1.0\n",
    "\n",
    "        # dist_to_goal = np.sqrt((self.goal_x - self.x) ** 2 + (self.goal_y - self.y) ** 2)\n",
    "\n",
    "        if action not in self.get_legal_actions(): # If action is illegal, stay at current position and discount reward by 1\n",
    "            action = 0\n",
    "            reward -= 1\n",
    "\n",
    "        self.x, self.y = self.get_next_position(action)\n",
    "        \n",
    "        # Update agent states\n",
    "        self.timestep += 1\n",
    "        self.around = torch.tensor(get_local_maze_information(self.y, self.x))\n",
    "        self.update_maze()\n",
    "\n",
    "        if self.game_end():\n",
    "            return 0.0\n",
    "        \n",
    "        # new_dist_to_goal = np.sqrt((self.goal_x - self.x) ** 2 + (self.goal_y - self.y) ** 2)\n",
    "        # if new_dist_to_goal < dist_to_goal:\n",
    "        #     reward += 1\n",
    "        # else:\n",
    "        #     reward -= 1\n",
    "        \n",
    "        return reward\n",
    "    \n",
    "    def game_end(self):\n",
    "        \"\"\"Return True if agent reaches the bottom right corner\"\"\"\n",
    "        if self.x == self.goal_x and self.y == self.goal_y: # 201 - 1 - wall\n",
    "            return True\n",
    "        return False\n",
    "    \n",
    "    def restart(self, x=1, y=1):\n",
    "        \"\"\"\n",
    "        Move the agent to the starting position and reset all agent states related to the environment.\n",
    "        Note that some fire might remain in the maze because we cannot call load_maze() again, \n",
    "        but they should be far away from the starting point so it does not really matter.\n",
    "        \"\"\"\n",
    "        self.timestep = 0\n",
    "        # self.maze = torch.zeros((self.maze_size, self.maze_size, 2))\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        # Make initial observation\n",
    "        self.around = torch.tensor(get_local_maze_information(self.y, self.x))\n",
    "        self.update_maze()\n",
    "    \n",
    "    def print_surroundings(self):\n",
    "        \"\"\"Print out the surrounding 9 positions.\"\"\"\n",
    "        for y in range(3):\n",
    "            for x in range(3):\n",
    "                if x == 1 and y == 1:\n",
    "                    print(\"X\", end=\" \")\n",
    "                    continue\n",
    "                if self.around[y][x][0] == 0:\n",
    "                    print(u\"\\u2588\", end=\" \")\n",
    "                if self.around[y][x][0] == 1 and self.around[y][x][1] > 0:\n",
    "                    print(self.around[y][x][1].item(), end=\" \")\n",
    "                if self.around[y][x][0] == 1 and self.around[y][x][1] == 0:\n",
    "                    print(\" \", end=\" \")\n",
    "            print()\n",
    "\n",
    "    def print_maze(self, x_low=0, y_low=0, size=15):\n",
    "        \"\"\"Print out the whole maze\"\"\"\n",
    "        for y in range(y_low, y_low+size):\n",
    "            for x in range(x_low, x_low+size):\n",
    "                if x == self.x and y == self.y:\n",
    "                    print(\"X\", end=\" \")\n",
    "                    continue\n",
    "                if self.maze[y][x][0] == 0:\n",
    "                    print(u\"\\u2588\", end=\" \")\n",
    "                if self.maze[y][x][0] == 1 and self.maze[y][x][1] > 0:\n",
    "                    print(self.maze[y][x][1].int().item(), end=\" \")\n",
    "                if self.maze[y][x][0] == 1 and self.maze[y][x][1] == 0:\n",
    "                    print(\" \", end=\" \")\n",
    "            print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2884,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def conv2d_size_out(size, kernel_size=3, stride=1):\n",
    "#     return (size - (kernel_size - 1) - 1) // stride + 1\n",
    "# conv_out_size = conv2d_size_out(conv2d_size_out(33))\n",
    "\n",
    "class FeatureNetwork(nn.Module):\n",
    "    def __init__(self, in_channel, out_channel):\n",
    "        \"\"\"out_channel: number of output channels in the last convolutional layer before being flattened and returned\"\"\"\n",
    "        super().__init__()\n",
    "        self.conv1 = nn.Conv2d(in_channel, 16, 3, padding=\"same\")\n",
    "        self.conv2 = nn.Conv2d(16, out_channel, 3, padding=\"same\")\n",
    "\n",
    "    def forward(self, state):\n",
    "        feature = F.relu(self.conv1(state))\n",
    "        feature = F.relu(self.conv2(feature))\n",
    "        feature = feature.view(feature.shape[0], -1)\n",
    "        return feature\n",
    "\n",
    "class QNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to estimate the action-values.\n",
    "    Input s, output q(s,a) for all a.\n",
    "    \"\"\"\n",
    "    def __init__(self, feature_dim, num_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim, 256)\n",
    "        self.fc2 = nn.Linear(256, num_action)\n",
    "    \n",
    "    def forward(self, feature):\n",
    "        output = F.relu(self.fc1(feature))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class NextStateNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to estimate the action-values.\n",
    "    Input s, output q(s,a) for all a.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim+1, 256)\n",
    "        self.fc2 = nn.Linear(256, feature_dim)\n",
    "\n",
    "    def forward(self, feature_action):\n",
    "        output = F.relu(self.fc1(feature_action))\n",
    "        output = self.fc2(output)\n",
    "        return output\n",
    "\n",
    "\n",
    "class ActionNetwork(nn.Module):\n",
    "    \"\"\"\n",
    "    Neural network to estimate the action-values.\n",
    "    Input s, output q(s,a) for all a.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, feature_dim, num_action):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(feature_dim*2, 256)\n",
    "        self.fc2 = nn.Linear(256, num_action)\n",
    "\n",
    "    def forward(self, feature_action):\n",
    "        output = F.relu(self.fc1(feature_action))\n",
    "        output = self.fc2(output)\n",
    "        return output\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2885,
   "metadata": {},
   "outputs": [],
   "source": [
    "class QLearningAgent:\n",
    "    \"\"\"\n",
    "    Agent that learns the optimal path to solve the maze using Q-Learning.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, env, target_update=10, batch_size=128, input_size=33, model=None, device='cpu'):\n",
    "        # Environment\n",
    "        self.env = env\n",
    "        self.x = env.x\n",
    "        self.y = env.y\n",
    "\n",
    "        # For Exploration\n",
    "        self.visited_times = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "        self.state_action_visited_times = torch.zeros((self.env.maze_size, self.env.maze_size, self.env.num_action))\n",
    "        self.last_visited_timestep = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "\n",
    "        # Training set up\n",
    "        self.batch_size = batch_size\n",
    "        self.target_update = target_update # Timesteps between target network update\n",
    "        self.input_size = input_size # Size of feature_net input\n",
    "        self.out_channel = 32\n",
    "        self.feature_dim = self.out_channel * self.input_size ** 2\n",
    "        self.device = device\n",
    "        self.replay_buffer = [] # list of tuples: (state, action, next_state, reward, done)\n",
    "\n",
    "        # Define networks\n",
    "        self.feature_net = FeatureNetwork(self.get_state(self.y, self.x).shape[0], self.out_channel).to(device)\n",
    "        self.feature_optimizer = optim.Adam(self.feature_net.parameters(), lr=1e-4)\n",
    "        self.q_net = QNetwork(feature_dim=self.feature_dim, num_action=self.env.num_action).to(device)\n",
    "        self.q_optimizer = optim.Adam(self.q_net.parameters(), lr=1e-4)\n",
    "        self.next_state_net = NextStateNetwork(feature_dim=self.feature_dim).to(device)\n",
    "        self.next_state_optimizer = optim.Adam(self.next_state_net.parameters(), lr=1e-4)\n",
    "        self.action_net = ActionNetwork(feature_dim=self.feature_dim, num_action=self.env.num_action).to(device)\n",
    "        self.action_optimizer = optim.Adam(self.action_net.parameters(), lr=1e-4)\n",
    "        \n",
    "        if model is not None:\n",
    "            model_info = torch.load(model, map_location=self.device)\n",
    "            self.feature_net.load_state_dict(model_info[\"feature_net\"])\n",
    "            self.feature_optimizer.load_state_dict(model_info[\"feature_optimizer\"])\n",
    "            self.q_net.load_state_dict(model_info[\"q_net\"])\n",
    "            self.q_optimizer.load_state_dict(model_info[\"q_optimizer\"])\n",
    "            self.next_state_net.load_state_dict(model_info[\"next_state_net\"])\n",
    "            self.next_state_optimizer.load_state_dict(model_info[\"next_state_optimizer\"])\n",
    "            self.action_net.load_state_dict(model_info[\"action_net\"])\n",
    "            self.action_optimizer.load_state_dict(model_info[\"action_optimizer\"])\n",
    "        \n",
    "        self.target_feature_net = FeatureNetwork(self.get_state(self.y, self.x).shape[0], self.out_channel).to(device)\n",
    "        self.target_feature_net.load_state_dict(self.feature_net.state_dict())\n",
    "        self.target_feature_net.eval()\n",
    "        self.target_net = QNetwork(feature_dim=self.feature_dim, num_action=self.env.num_action).to(device)\n",
    "        self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "        self.target_net.eval()\n",
    "    \n",
    "    def save_maze_policy(self, path):\n",
    "        \"\"\"Save the current policy as an image\"\"\"\n",
    "        policy_image = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "        for y in range(self.env.maze_size):\n",
    "            for x in range(self.env.maze_size):\n",
    "                if self.env.maze[y, x, 0]:  # not a wall\n",
    "                    # Comput the best action\n",
    "                    state = self.get_state(y, x).to(self.device)\n",
    "                    action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                    best_action = torch.argmax(action_values)\n",
    "                    policy_image[y, x] = best_action\n",
    "\n",
    "        # Plot heatmap to show frequency of visiting each position\n",
    "        newcmap = ListedColormap(['black', 'blue', 'red', 'yellow', 'white'])\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        im = ax.imshow(policy_image, cmap=newcmap)\n",
    "        fig.colorbar(im)\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_visit_frequency(self, path):\n",
    "        # Save a heatmap showing frequency of visiting each position\n",
    "        fig, ax = plt.subplots(figsize=(20, 20))\n",
    "        im = ax.imshow(self.visited_times, cmap='gray')\n",
    "        fig.colorbar(im)\n",
    "        plt.savefig(path)\n",
    "        plt.close(fig)\n",
    "\n",
    "    def save_explored_maze(self, path):\n",
    "        \"\"\"Save the explored positions as an image\"\"\"\n",
    "        plt.figure(figsize=(20, 20))\n",
    "        plt.imshow((self.visited_times > 0), cmap=\"gray\")\n",
    "        plt.savefig(path)\n",
    "        plt.close()\n",
    "    \n",
    "    def print_state(self, state):\n",
    "        \"\"\"Print out the whole maze\"\"\"\n",
    "        for y in range(state.shape[1]):\n",
    "            for x in range(state.shape[2]):\n",
    "                if state[0, y, x] == 1:\n",
    "                    print(\"X\", end=\" \")\n",
    "                    continue\n",
    "                if state[3, y, x] == 1:\n",
    "                    print(\"G\", end=\" \")\n",
    "                    continue\n",
    "                if state[1, y, x] == 0:\n",
    "                    print(u\"\\u2588\", end=\" \")\n",
    "                if state[2, y, x] > 0:\n",
    "                    print(state[2, y, x].item(), end=\" \")\n",
    "                if state[1, y, x] == 1 and state[2, y, x] == 0:\n",
    "                    print(\" \", end=\" \")\n",
    "                \n",
    "            print()\n",
    "\n",
    "    # def explore_maze(self, direction):\n",
    "    #     self.restart(x=1, y=1)\n",
    "    #     if direction == \"right\":\n",
    "    \n",
    "    def get_state(self, y, x, goal_x=None, goal_y=None):\n",
    "        \"\"\"Return a tensor representing the current state\"\"\"\n",
    "        if goal_x is None:\n",
    "            goal_x = self.env.goal_x\n",
    "        if goal_y is None:\n",
    "            goal_y = self.env.goal_y\n",
    "        # state1 = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "        # state1[self.y][self.x] = 1.0\n",
    "        # state1 = state1.view(-1)\n",
    "\n",
    "        # state2 = []\n",
    "        # for y in range(3):\n",
    "        #     for x in range(3):\n",
    "        #         if x == 1 and y == 1:  # Current Position\n",
    "        #             continue\n",
    "        #         state2.append(self.around[y][x][0])\n",
    "        #         state2.append(self.around[y][x][1])\n",
    "        # state2 = torch.tensor(state2).float()\n",
    "        # return torch.cat([state1, state2])\n",
    "\n",
    "        # # CNN whole map as input\n",
    "        # state = torch.zeros((4, self.env.maze_size, self.env.maze_size))\n",
    "        # state[0, self.y, self.x] = 1  # Current position\n",
    "        # state[1] = self.env.maze[:, :, 0]  # Walls\n",
    "        # state[2] = self.env.maze[:, :, 1]  # Fires\n",
    "        # state[3, self.env.goal_y, self.env.goal_x] = 1  # Goal position\n",
    "\n",
    "        # Local CNN with nearby maze info as input\n",
    "        # Find the top left corner of the window, such that it is not out of the maze\n",
    "        x_left = max(0, x - int((self.input_size+1)/2))\n",
    "        x_left = min(x_left, self.env.maze_size - self.input_size)\n",
    "        y_top = max(0, y - int((self.input_size+1)/2))\n",
    "        y_top = min(y_top, self.env.maze_size - self.input_size)\n",
    "\n",
    "        state = torch.zeros((6, self.input_size, self.input_size))\n",
    "        state[0, y - y_top, x - x_left] = 1 # Current position\n",
    "        state[1] = self.env.maze[y_top:y_top+self.input_size, x_left:x_left+self.input_size, 0]  # Walls\n",
    "        state[2] = self.env.maze[y_top:y_top+self.input_size, x_left:x_left+self.input_size, 1]  # Fires\n",
    "        if goal_y - y_top >= 0 and goal_y - y_top < self.input_size and goal_x - x_left >= 0 and goal_x - x_left < self.input_size:\n",
    "            state[3, goal_y - y_top, goal_x - x_left] = 1  # Goal position\n",
    "        # state[4] = torch.full((input_size, input_size), y)\n",
    "        # state[5] = torch.full((input_size, input_size), x)\n",
    "        state[4] = torch.full((self.input_size, self.input_size), y_top)\n",
    "        state[5] = torch.full((self.input_size, self.input_size), x_left)\n",
    "        # state[6] = torch.full((self.input_size, self.input_size), goal_x)\n",
    "        # state[7] = torch.full((self.input_size, self.input_size), goal_y)\n",
    "\n",
    "        return state\n",
    "    \n",
    "    def restart(self, x=1, y=1):\n",
    "        \"\"\"Restart the environment. Called everytime before starting a new episode.\"\"\"\n",
    "        self.env.restart(x=x, y=y)\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.visited_times = torch.zeros((self.env.maze_size, self.env.maze_size))\n",
    "        self.state_action_visited_times = torch.zeros((self.env.maze_size, self.env.maze_size, 5))\n",
    "        \n",
    "    def is_goal_state(self, state):\n",
    "        \"\"\"Return True if the state is the goal state\"\"\"\n",
    "        # if state[self.env.goal_y*self.env.maze_size + self.env.goal_x]:\n",
    "        if torch.equal(state[0], state[3]):\n",
    "            return True\n",
    "        return False\n",
    "\n",
    "    def get_action(self, state, mode=\"full_explore\", epsilon=0.8):\n",
    "        \"\"\"Given a state, return an action according to the policy.\"\"\"\n",
    "        state = state.to(self.device)\n",
    "\n",
    "        legal_actions = self.env.get_legal_actions()\n",
    "        \n",
    "        if mode == \"full_explore\":\n",
    "            action_to_least_visited_neighbour = None\n",
    "            min_visited_times = torch.inf\n",
    "            for action in range(5):\n",
    "                next_x, next_y = self.env.get_next_position(action)\n",
    "                if self.visited_times[next_y, next_x] <= min_visited_times:\n",
    "                    # Legal action, record it\n",
    "                    if action in legal_actions:\n",
    "                        min_visited_times = self.visited_times[next_y, next_x]\n",
    "                        action_to_least_visited_neighbour = action\n",
    "                    # Illegal action but it is due to fire: wait there\n",
    "                    elif self.env.maze[next_y, next_x, 0]:\n",
    "                        action_to_least_visited_neighbour = 0\n",
    "                        min_visited_times = self.visited_times[next_y, next_x]\n",
    "\n",
    "            return action_to_least_visited_neighbour\n",
    "\n",
    "        elif mode == \"warm_up\": # Go to a position closest to the goal most of the time, pick random action otherwise\n",
    "            action_closest_to_goal = None\n",
    "            max_visited_timestep = -torch.inf\n",
    "            for action in range(5):\n",
    "                if action in legal_actions:\n",
    "                    next_x, next_y = self.env.get_next_position(action)\n",
    "                    if self.last_visited_timestep[next_y, next_x] > max_visited_timestep:\n",
    "                        max_visited_timestep = self.last_visited_timestep[next_y, next_x]\n",
    "                        action_closest_to_goal = action\n",
    "            if random.random() < 0.5:\n",
    "                return random.randint(0,4)\n",
    "            return action_closest_to_goal\n",
    "\n",
    "        elif mode == \"e_greedy\":\n",
    "            # Epsilon Greedy\n",
    "            if random.random() < epsilon:\n",
    "                # Return random action\n",
    "                return random.randint(0,4)\n",
    "            else:\n",
    "                # Return Best Action\n",
    "                action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                max_action = torch.argmax(action_values).item()\n",
    "                return max_action\n",
    "        else:\n",
    "            raise ValueError(f\"Unknown mode: {mode}\")\n",
    "\n",
    "        # # UCB1\n",
    "        # action_values = self.q_net(state)\n",
    "        # # print(action_values)\n",
    "        # for action in range(5):\n",
    "        #     if self.state_action_visited_times[self.y, self.x, action] == 0:\n",
    "        #         return action\n",
    "        # ucb = action_values + epsilon * torch.sqrt(torch.log(torch.tensor(self.env.timestep+1)) / (self.state_action_visited_times[self.y, self.x, :]))\n",
    "        # action = torch.argmax(ucb).item()\n",
    "        # return action\n",
    "\n",
    "    \n",
    "    def update_network(self, target=\"target_net\", gamma=0.99):\n",
    "        \"\"\"Update network weights\"\"\"\n",
    "        batch = random.sample(self.replay_buffer, min(len(self.replay_buffer), self.batch_size))\n",
    "        state_batch = torch.stack([data[0] for data in batch]).to(self.device)\n",
    "        action_batch = torch.tensor([data[1] for data in batch]).to(self.device)\n",
    "        next_state_batch = torch.stack([data[2] for data in batch]).to(self.device)\n",
    "        reward_batch = torch.tensor([data[3] for data in batch]).to(self.device)\n",
    "        done = torch.tensor([data[4] for data in batch]).to(self.device)\n",
    "\n",
    "        # #----- Compute the loss of next state feature prediction -----#\n",
    "        # # Prediction\n",
    "        # state_feature = self.feature_net(state_batch)\n",
    "        # state_feature_concat_action = torch.cat((state_feature, action_batch.unsqueeze(1)), 1)\n",
    "        # next_state_feature_prediction = self.next_state_net(state_feature_concat_action)\n",
    "        # # Target\n",
    "        # next_state_feature = self.feature_net(next_state_batch)\n",
    "\n",
    "        # # MSE loss\n",
    "        # next_state_criterion = nn.MSELoss()\n",
    "        # next_state_loss = next_state_criterion(next_state_feature_prediction, next_state_feature)\n",
    "        next_state_loss = 0\n",
    "\n",
    "        # #----- Compute the loss of action prediction -----#\n",
    "        # # Prediction\n",
    "        # action_prediction = self.action_net(torch.cat((state_feature, next_state_feature), 1))\n",
    "\n",
    "        # # Target (Set action to 0 if agent stays in the same position. This assumes that an optimal agent does not take illegal action)\n",
    "        # y_top = state_batch[:, 4, 0, 0]\n",
    "        # x_left = state_batch[:, 5, 0, 0]\n",
    "        # agent_position = state_batch[:, 0].nonzero()[:, 1:]\n",
    "        # agent_position[:, 0] += y_top.long()\n",
    "        # agent_position[:, 1] += x_left.long()\n",
    "        # agent_position = agent_position[:, 0] * self.env.maze_size + agent_position[:, 1]\n",
    "\n",
    "        # y_top = next_state_batch[:, 4, 0, 0]\n",
    "        # x_left = next_state_batch[:, 5, 0, 0]\n",
    "        # next_state_agent_position = next_state_batch[:, 0].nonzero()[:, 1:]\n",
    "        # next_state_agent_position[:, 0] += y_top.long()\n",
    "        # next_state_agent_position[:, 1] += x_left.long()\n",
    "        # next_state_agent_position = next_state_agent_position[:, 0] * self.env.maze_size + next_state_agent_position[:, 1]\n",
    "        # is_same_position = (agent_position == next_state_agent_position).int()\n",
    "        \n",
    "        # # Cross Entropy loss\n",
    "        # action_criterion = nn.CrossEntropyLoss()\n",
    "        # action_loss = action_criterion(action_prediction, action_batch * (1 - is_same_position))\n",
    "        action_loss = 0\n",
    "        \n",
    "        #----- Compute the loss of q values prediction -----#\n",
    "        # Prediction (Only update against the action that the agent took)\n",
    "        q_values_prediction = self.q_net(self.feature_net(state_batch)) # q value for all actions\n",
    "        q_values_prediction = q_values_prediction[torch.arange(q_values_prediction.shape[0]), action_batch]  # q value for the selected action\n",
    "\n",
    "        # Target (Next state's best q value)\n",
    "        # non_goal_state_indices = [not self.is_goal_state(data[2]) for data in batch]\n",
    "        # next_state_batch = torch.stack([data[2] for data in batch if not self.is_goal_state(data[2])])\n",
    "        if target == \"target_net\" or True:\n",
    "            # Compute indices where next state is the goal state\n",
    "            next_q_values = self.target_net(self.target_feature_net(next_state_batch))\n",
    "            next_state_values, _ = torch.max(next_q_values, dim=1)\n",
    "            next_state_values = next_state_values.detach() # Stop Gradient\n",
    "            \n",
    "        elif target == \"prior\":\n",
    "            y_top = next_state_batch[:, 4, 0, 0]\n",
    "            x_left = next_state_batch[:, 5, 0, 0]\n",
    "            next_state_agent_position = next_state_batch[:, 0].nonzero()[:, 1:]\n",
    "            next_state_agent_position[:, 0] += y_top.long()\n",
    "            next_state_agent_position[:, 1] += x_left.long()\n",
    "            next_state_values = self.last_visited_timestep[next_state_agent_position[:, 0], next_state_agent_position[:, 1]] - torch.max(self.last_visited_timestep)\n",
    "            next_state_values = next_state_values.to(self.device)\n",
    "            \n",
    "        \n",
    "        # print(\"state:\",(state_batch[:, 0] == 1).nonzero(as_tuple=False)[:, 1:])\n",
    "        # print(\"action:\", action_batch)\n",
    "        # print(\"q_value:\", q_values_prediction)\n",
    "        # print(\"reward:\",reward_batch)\n",
    "        # print(\"Next_state_values:\",next_state_values)\n",
    "\n",
    "        # print(\"target:\", reward_batch + gamma * next_state_values)\n",
    "        # print(\"difference:\", reward_batch + gamma * next_state_values - q_values_prediction)\n",
    "\n",
    "        # w = 1\n",
    "        # intrinsic_reward = w * 0.5 * torch.sum((next_state_feature_prediction - next_state_feature) ** 2, dim=1)\n",
    "        # print(\"intrinsic_reward:\", intrinsic_reward)\n",
    "        # print(\"target:\", intrinsic_reward + reward_batch + gamma * next_state_values * (1 - done))\n",
    "        intrinsic_reward = 0\n",
    "        \n",
    "        q_criterion = nn.SmoothL1Loss()\n",
    "        q_loss = q_criterion(q_values_prediction,  0 * intrinsic_reward + reward_batch + gamma * next_state_values * (1 - done)) # Multiply by (1 - done) to set target value of goal state to 0\n",
    "        \n",
    "        #----- Backprop all losses -----#\n",
    "        self.feature_optimizer.zero_grad()\n",
    "        self.q_optimizer.zero_grad()\n",
    "        # self.next_state_optimizer.zero_grad()\n",
    "        # self.action_optimizer.zero_grad()\n",
    "\n",
    "        beta = 0.5\n",
    "        lamb = 1 # 0.1\n",
    "        loss = beta * next_state_loss + (1 - beta) * action_loss + lamb * q_loss\n",
    "        loss.backward()\n",
    "\n",
    "        # torch.nn.utils.clip_grad_norm_(self.feature_net.parameters(), 0.5)\n",
    "        # torch.nn.utils.clip_grad_norm_(self.q_net.parameters(), 0.5)\n",
    "        \n",
    "        self.feature_optimizer.step()\n",
    "        self.q_optimizer.step()\n",
    "        # self.next_state_optimizer.step()\n",
    "        # self.action_optimizer.step()\n",
    "        \n",
    "        return loss.cpu().item(), beta * next_state_loss.cpu().item(), (1 - beta) * action_loss.cpu().item(), lamb * q_loss.cpu().item()\n",
    "\n",
    "    def run_episode(self, start_x=1, start_y=1, mode=\"e_greedy\", epsilon=0.8, gamma=0.99, max_timestep=20000, update=True, save=True, display=False):\n",
    "        \"\"\"\n",
    "        Run a single episode.\n",
    "        Parameters: \n",
    "            - epsilon: parameter for epsilon-greedy\n",
    "            - save: save trajectory to replay buffer if true\n",
    "            - display: display surroundings and legal actions during the episode if true\n",
    "        \"\"\"\n",
    "        # Start from start state\n",
    "        self.restart(x=start_x, y=start_y)\n",
    "\n",
    "        total_reward = 0\n",
    "        trajectory = []\n",
    "        episode_loss = {\"total\": [], \"next_state\": [], \"action\": [], \"q\": []}\n",
    "\n",
    "        # For displaying the first timestep of an episode only\n",
    "        action = 0\n",
    "        reward = 0\n",
    "\n",
    "        # Additional goal state for hindisght experience replay (Use the state closest to the env goal state)\n",
    "        additional_goal_x, additional_goal_y = 1, 1\n",
    "\n",
    "        start_time = time.perf_counter()\n",
    "        while not self.env.game_end() and self.env.timestep < max_timestep:\n",
    "            if display:\n",
    "                # Print current state\n",
    "                print(f\"Timestep: {self.env.timestep} | Position: ({self.x},{self.y}) | Epsilon: {epsilon:.4f} | Last Action: {action} | Last Reward: {reward:4f} | Time Used: {time.perf_counter() - start_time:.2f}\")\n",
    "                # self.print_surroundings()\n",
    "                self.env.print_maze(x_low=max(0, self.x - 15), x_high=min(self.env.maze_size, self.x+15), y_low=max(0, self.y - 5), y_high=min(self.env.maze_size, self.y+5))\n",
    "                state = self.get_state(self.y, self.x).to(self.device)\n",
    "                # self.print_state(state)\n",
    "                action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                print(action_values.detach().cpu().numpy())\n",
    "            else:\n",
    "                if self.env.timestep % 500 == 0:\n",
    "                    # Print current state\n",
    "                    print(f\"Timestep: {self.env.timestep} | Position: ({self.x},{self.y}) | Epsilon: {epsilon:.4f} | Last Action: {action} | Last Reward: {reward:4f} | Time Used: {time.perf_counter() - start_time:.2f}\")\n",
    "                    state = self.get_state(self.y, self.x).to(self.device)\n",
    "                    action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                    print(action_values.detach().cpu().numpy())\n",
    "            \n",
    "            # Store current state info\n",
    "            previous_y, previous_x = self.y, self.x\n",
    "            no_fire_around = self.env.around[[0,1,1,2], [1,0,2,1], 1].sum().item() == 0\n",
    "\n",
    "            # Select and make action\n",
    "            state = self.get_state(self.y, self.x)\n",
    "            action = self.get_action(state, mode=mode)\n",
    "            reward = self.env.make_action(action)\n",
    "\n",
    "            # Update agent's position and exploration info\n",
    "            self.x, self.y = self.env.x, self.env.y\n",
    "            self.visited_times[self.y, self.x] += 1\n",
    "            # self.state_action_visited_times[previous_y, previous_x, action] += 1\n",
    "            if mode == \"full_explore\":\n",
    "                self.last_visited_timestep[self.y, self.x] = self.env.timestep\n",
    "            if mode == \"warm_up\" and self.last_visited_timestep[self.y, self.x] == 0 and self.y != 1 and self.x != 1: # Set timestep of unvisited position as the visited position - 1. (1,1) is the start state which should have value 0.\n",
    "                self.last_visited_timestep[self.y, self.x] = self.last_visited_timestep[previous_y, previous_x] - 1\n",
    "            \n",
    "            # Get next state\n",
    "            next_state = self.get_state(self.y, self.x)\n",
    "            \n",
    "            # Reward Shaping\n",
    "            # Add a reward between [-1, 0], penalize items that are visited long time ago before reaching the goal state during full explore\n",
    "            # if torch.max(self.last_visited_timestep) > 0:\n",
    "            #     reward += 1 * (self.last_visited_timestep[self.y, self.x] / torch.max(self.last_visited_timestep) - 1)\n",
    "            # # Penalize going to previous state unless the action is stay\n",
    "            # if trajectory and torch.equal(self.get_state(self.y, self.x)[0], trajectory[-1][0][0]) and action != 0:\n",
    "            #     reward -= 1\n",
    "\n",
    "            # If you stay while there is no fire around, you get -0.5 reward. This is to solve the problem where the agent tends to stay to get more reward. \n",
    "            # For example, q value of staying is 1, but max q of all other next states are < 1. Both rewards are -1 because they are not goal state, so the agent prefers to stay.\n",
    "            if action == 0 and no_fire_around:\n",
    "                reward -= 0.5\n",
    "            \n",
    "            total_reward += reward\n",
    "\n",
    "            # Save to trajectory\n",
    "            trajectory.append([state, action, next_state, reward, int(self.env.game_end())])\n",
    "\n",
    "            # Save to replay buffer\n",
    "            if save:\n",
    "                self.replay_buffer.append([state, action, next_state, reward, int(self.env.game_end())])\n",
    "\n",
    "                # Update additional goal position\n",
    "                if self.last_visited_timestep[additional_goal_y, additional_goal_x] < self.last_visited_timestep[self.y, self.x]:\n",
    "                    additional_goal_x, additional_goal_y = self.x, self.y\n",
    "\n",
    "                # if len(self.replay_buffer) > 50000:\n",
    "                #     self.replay_buffer = self.replay_buffer[len(self.replay_buffer)-50000:]\n",
    "                \n",
    "            # Update networks\n",
    "            if update:\n",
    "                if mode == \"warm_up\":\n",
    "                    # Update towards prior (last_visited_timestep - maximum of last_visited_timestep)\n",
    "                    total_loss, next_state_loss, action_loss, q_loss = self.update_network(target=\"prior\", gamma=gamma)\n",
    "                else:\n",
    "                    total_loss, next_state_loss, action_loss, q_loss = self.update_network(gamma=gamma)\n",
    "\n",
    "                episode_loss[\"total\"].append(total_loss)\n",
    "                episode_loss[\"next_state\"].append(next_state_loss)\n",
    "                episode_loss[\"action\"].append(action_loss)\n",
    "                episode_loss[\"q\"].append(q_loss)\n",
    "\n",
    "                if self.env.timestep % self.target_update == 0: # Update target network\n",
    "                    self.target_net.load_state_dict(self.q_net.state_dict())\n",
    "                    self.target_feature_net.load_state_dict(self.feature_net.state_dict())\n",
    "\n",
    "        # if save and (additional_goal_x != self.env.goal_x or additional_goal_y != self.env.goal_y):\n",
    "        #     # Hindsight experience replay\n",
    "        #     # additional_goal_x, additional_goal_y = self.x, self.y # (Use the final position as the goal)\n",
    "        #     print(\"Additional Goal:\", additional_goal_x, additional_goal_y)\n",
    "        #     for t in trajectory:\n",
    "        #         new_t = []\n",
    "        #         for i in range(5):\n",
    "        #             if i == 0 or i == 2: # Update state's goal channel with new goal position\n",
    "        #                 state = t[i].clone()\n",
    "        #                 y_top = state[4, 0, 0].int()\n",
    "        #                 x_left = state[5, 0, 0].int()\n",
    "        #                 new_goal_channel = torch.zeros((self.input_size, self.input_size))\n",
    "        #                 if additional_goal_y - y_top >= 0 and additional_goal_y - y_top < self.input_size and additional_goal_x - x_left >= 0 and additional_goal_x - x_left < self.input_size:\n",
    "        #                     new_goal_channel[additional_goal_y - y_top, additional_goal_x - x_left] = 1  # Additional goal position\n",
    "        #                 state[3] = new_goal_channel\n",
    "        #                 # state[6] = torch.full((self.input_size, self.input_size), additional_goal_x)\n",
    "        #                 # state[7] = torch.full((self.input_size, self.input_size), additional_goal_y)\n",
    "        #                 new_t.append(state)\n",
    "        #             elif i == 1: # Same action\n",
    "        #                 new_t.append(t[i])\n",
    "        #             elif i == 3: # Check if next_state is a goal state according to the new goal position, and update reward\n",
    "        #                 reward = 0 if self.is_goal_state(new_t[2]) else t[3]\n",
    "        #                 new_t.append(reward)\n",
    "        #             elif i == 4:  # Check if next_state is a goal state according to the new goal position, and update done\n",
    "        #                 done = 1 if self.is_goal_state(new_t[2]) else 0\n",
    "        #                 new_t.append(done)\n",
    "                \n",
    "        #         self.replay_buffer.append(new_t)\n",
    "\n",
    "        #     # if len(self.replay_buffer) > 50000:\n",
    "        #     #     self.replay_buffer = self.replay_buffer[len(self.replay_buffer)-50000:]\n",
    "    \n",
    "        print(f\"Timestep: {self.env.timestep} | Position: ({self.x},{self.y}) | Epsilon: {epsilon:.4f} | Last Action: {action} | Last Reward: {reward:4f} | Time Used: {time.perf_counter() - start_time:.2f}\")\n",
    "        \n",
    "        # Return total reward\n",
    "        return self.env.timestep, total_reward, trajectory, episode_loss\n",
    "\n",
    "    def train(self, full_explore=1, warm_up=0, e_greedy=1, epsilon=0.8, min_epsilon=0.1, move_start_position=False, previous_training_history_path=None, previous_replay_buffer_path=None):\n",
    "        timesteps = []\n",
    "        rewards = []\n",
    "        trajectories = []\n",
    "        losses = {\"total\": [], \"next_state\": [], \"action\": [], \"q\": []}\n",
    "        if previous_training_history_path is not None:\n",
    "            previous_training_history = torch.load(previous_training_history_path, map_location=self.device)\n",
    "            timesteps = previous_training_history[\"timesteps\"]\n",
    "            rewards = previous_training_history[\"rewards\"]\n",
    "            losses = previous_training_history[\"losses\"]\n",
    "        \n",
    "        if previous_replay_buffer_path is not None:\n",
    "            previous_replay_buffer = torch.load(previous_replay_buffer_path, map_location=self.device)\n",
    "            self.replay_buffer = previous_replay_buffer\n",
    "        \n",
    "        print(\"Initial number of data in replay buffer:\", len(self.replay_buffer))\n",
    "\n",
    "        for i in range(full_explore): # To explore the maze, initialize self.last_visited_timestep, update the maze\n",
    "            print(f\"Starts Full Explore Episode {i}\")\n",
    "            timestep, reward, trajectory, episode_loss = self.run_episode(mode=\"full_explore\", display=False, save=False, update=False)\n",
    "            # timesteps.append(timestep)\n",
    "            # rewards.append(reward)\n",
    "            # trajectories.append(trajectory)\n",
    "            # for key in losses:\n",
    "            #     losses[key].append(episode_loss[key])\n",
    "\n",
    "            self.save_visit_frequency(f\"maze/frequency/f_{i}.png\")\n",
    "            self.save_explored_maze(f\"maze/explored/f_{i}.png\")\n",
    "            self.save_maze_policy(f\"maze/policy/f_{i}.png\")\n",
    "            print()\n",
    "        \n",
    "        # A sequence of starting position, sorted according to the distance from goal\n",
    "        starting_position = []\n",
    "        if move_start_position:\n",
    "            for y in range(self.env.maze_size):\n",
    "                for x in range(self.env.maze_size):\n",
    "                    starting_position.append((y, x))\n",
    "            starting_position.sort(key=lambda p: self.last_visited_timestep[p[0], p[1]], reverse=True) # Sort according to the distance to goal state, in ascending order (goal position is the first element)\n",
    "            for i, p in enumerate(starting_position):\n",
    "                if self.last_visited_timestep[p[0], p[1]] == 0:\n",
    "                    break\n",
    "            starting_position = starting_position[1:i]  # Ignore goal state and all unexplored positions\n",
    "            starting_position.append((1,1)) # Add the starting point back because it has a 0 last visited timestep as well\n",
    "        else:\n",
    "            starting_position = [(1,1)]\n",
    "        \n",
    "        print(f\"Start running {warm_up} warm-up episodes and {len(starting_position) * e_greedy} q-learning episodes. {'With' if move_start_position else 'Without'} moving start position.\\n\")\n",
    "        # print(f\"Start running {len(starting_position) * warm_up} warm-up episodes and {len(starting_position) * e_greedy} q-learning episodes. {'With' if move_start_position else 'Without'} moving start position.\\n\")\n",
    "        \n",
    "        for i in range(warm_up):\n",
    "            st = time.perf_counter()\n",
    "            print(f\"Starts Warm Up Episode {i}\")\n",
    "            timestep, reward, trajectory, episode_loss = self.run_episode(mode=\"warm_up\", display=False, update=False)\n",
    "            timesteps.append(timestep)\n",
    "            rewards.append(reward)\n",
    "            # trajectories.append(trajectory)\n",
    "            for key in losses:\n",
    "                losses[key].append(episode_loss[key])\n",
    "\n",
    "            if i+1 % 20 == 0:\n",
    "                torch.save({\"timesteps\": timesteps, \"rewards\": rewards, \"losses\": losses}, \"training_histories.pth\")\n",
    "\n",
    "            if i+1 % 10 == 0:\n",
    "                self.save_visit_frequency(f\"maze/frequency/w_{i}.png\")\n",
    "                self.save_explored_maze(f\"maze/explored/w_{i}.png\")\n",
    "                self.save_maze_policy(f\"maze/policy/w_{i}.png\")\n",
    "\n",
    "            # for y in range(self.env.goal_y+1):\n",
    "            #     for x in range(self.env.goal_x+1):\n",
    "            #         if self.env.maze[y, x, 0] == 1:\n",
    "            #             state = self.get_state(y, x).to(self.device)\n",
    "            #             action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "            #             print(f\"Position: ({x},{y}) | Timestep Until Goal: {self.last_visited_timestep[y, x] - torch.max(self.last_visited_timestep)} | Action Values: {action_values.detach().cpu().numpy()}\")\n",
    "            print(\"Full episode:\", time.perf_counter() - st)\n",
    "            print()\n",
    "            \n",
    "        # Save model\n",
    "        torch.save({\n",
    "            \"feature_net\": self.feature_net.state_dict(),\n",
    "            \"q_net\": self.q_net.state_dict(),\n",
    "            \"next_state_net\": self.next_state_net.state_dict(),\n",
    "            \"action_net\": self.action_net.state_dict(),\n",
    "            \"feature_optimizer\": self.feature_optimizer.state_dict(),\n",
    "            \"q_optimizer\": self.q_optimizer.state_dict(),\n",
    "            \"next_state_optimizer\": self.next_state_optimizer.state_dict(),\n",
    "            \"action_optimizer\": self.action_optimizer.state_dict()\n",
    "        }, \"models/warm_up.pth\")\n",
    "\n",
    "        print(f\"Start running {len(starting_position) * e_greedy} q-learning episodes. {'With' if move_start_position else 'Without'} moving start position.\\n\")\n",
    "        for i, (y, x) in enumerate(starting_position):\n",
    "            if self.last_visited_timestep[self.env.goal_y, self.env.goal_x] - self.last_visited_timestep[y, x] < 10:\n",
    "                agent.target_update = 3\n",
    "            else:\n",
    "                agent.target_update = 10\n",
    "\n",
    "            for j in range(e_greedy):\n",
    "                st = time.perf_counter()\n",
    "                # max_timestep = 20000\n",
    "                max_timestep = 5 * (self.last_visited_timestep[self.env.goal_y, self.env.goal_x] - self.last_visited_timestep[y, x])\n",
    "                # max_timestep = min(200 * (i//10+1), 4000)\n",
    "                print(f\"Starts Q-Learning Episode {i*e_greedy+j}. Starting point: ({x},{y})\")\n",
    "                timestep, reward, trajectory, episode_loss = self.run_episode(start_x=x, start_y=y, mode=\"e_greedy\", epsilon=epsilon, display=False, max_timestep=max_timestep)\n",
    "                timesteps.append(timestep)\n",
    "                rewards.append(reward)\n",
    "                # trajectories.append(trajectory)\n",
    "                for key in losses:\n",
    "                    losses[key].append(episode_loss[key])\n",
    "\n",
    "                if (i*e_greedy+j) % 20 == 0:\n",
    "                    torch.save({\"timesteps\": timesteps, \"rewards\": rewards, \"losses\": losses}, \"training_histories.pth\")\n",
    "                \n",
    "                if (i*e_greedy+j+1) % 2000 == 0:\n",
    "                    torch.save({\"buffer\": self.replay_buffer}, \"replay_buffer.pth\")\n",
    "                \n",
    "                if j == e_greedy - 1:  # End of one starting point\n",
    "                    self.save_visit_frequency(f\"maze/frequency/q_{i*e_greedy+j+1}.png\")\n",
    "                    self.save_explored_maze(f\"maze/explored/q_{i*e_greedy+j+1}.png\")\n",
    "                    self.save_maze_policy(f\"maze/policy/q_{i*e_greedy+j+1}.png\")\n",
    "\n",
    "                # for y in range(self.env.goal_y+1):\n",
    "                #     for x in range(self.env.goal_x+1):\n",
    "                #         if self.env.maze[y, x, 0] == 1:\n",
    "                #             state = self.get_state(y, x).to(self.device)\n",
    "                #             action_values = self.q_net(self.feature_net(state.unsqueeze(0)))[0]\n",
    "                #             print(f\"Position: ({x},{y}) | Action Values: {action_values.detach().cpu(numpy()}\")\n",
    "\n",
    "                print(\"Full episode:\", time.perf_counter() - st)\n",
    "                # Epsilon decay\n",
    "                if epsilon > min_epsilon:\n",
    "                    epsilon -= 1e-3\n",
    "                print()\n",
    "\n",
    "        agent.target_update = 10\n",
    "\n",
    "        # Save model\n",
    "        torch.save({\n",
    "            \"feature_net\": self.feature_net.state_dict(),\n",
    "            \"q_net\": self.q_net.state_dict(),\n",
    "            \"next_state_net\": self.next_state_net.state_dict(),\n",
    "            \"action_net\": self.action_net.state_dict(),\n",
    "            \"feature_optimizer\": self.feature_optimizer.state_dict(),\n",
    "            \"q_optimizer\": self.q_optimizer.state_dict(),\n",
    "            \"next_state_optimizer\": self.next_state_optimizer.state_dict(),\n",
    "            \"action_optimizer\": self.action_optimizer.state_dict()\n",
    "        }, \"models/q_learning.pth\")\n",
    "\n",
    "        torch.save({\"timesteps\": timesteps, \"rewards\": rewards, \"losses\": losses}, \"/content/drive/My Drive/colab/maze_solver/training_histories.pth\")\n",
    "\n",
    "        return timesteps, rewards, trajectories, losses\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2886,
   "metadata": {},
   "outputs": [],
   "source": [
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "# env = Environment(199, 199)\n",
    "# env = Environment(28, 135)\n",
    "# env = Environment(48, 31)\n",
    "env = Environment(23, 9)\n",
    "agent = QLearningAgent(env, input_size=25, device=device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2887,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starts Full Explore Episode 0\n",
      "Timestep: 0 | Position: (1,1) | Epsilon: 0.8000 | Last Action: 0 | Last Reward: 0.000000 | Time Used: 0.00\n",
      "[ 0.00121204 -0.03221715  0.05059817 -0.04487094  0.01599444]\n",
      "Timestep: 63 | Position: (23,9) | Epsilon: 0.8000 | Last Action: 2 | Last Reward: 0.000000 | Time Used: 0.70\n",
      "Starts Q-Learning Episode 0\n",
      "Timestep: 0 | Position: (23,9) | Epsilon: 0.8000 | Last Action: 0 | Last Reward: 0.000000 | Time Used: 0.00\n",
      "[-0.04256147  0.01520971  0.01439995 -0.13770396  0.24608226]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-2887-4998d032b118>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtimesteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectories\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlosses\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfull_explore\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwarm_up\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0me_greedy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m30\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.8\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-2885-19a69b03974d>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, full_explore, warm_up, e_greedy, epsilon)\u001b[0m\n\u001b[1;32m    520\u001b[0m             \u001b[0;31m# max_timestep = min(200 * (i//10+1), 4000)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    521\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"Starts Q-Learning Episode {i}\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 522\u001b[0;31m             \u001b[0mtimestep\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrajectory\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepisode_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_episode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"e_greedy\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepsilon\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mepsilon\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdisplay\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_timestep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_timestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    523\u001b[0m             \u001b[0mtimesteps\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimestep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    524\u001b[0m             \u001b[0mrewards\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2885-19a69b03974d>\u001b[0m in \u001b[0;36mrun_episode\u001b[0;34m(self, mode, epsilon, gamma, max_timestep, update, save, display)\u001b[0m\n\u001b[1;32m    413\u001b[0m                     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"prior\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    414\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 415\u001b[0;31m                     \u001b[0mtotal_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction_loss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate_network\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mgamma\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    416\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    417\u001b[0m                 \u001b[0mepisode_loss\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"total\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtotal_loss\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-2885-19a69b03974d>\u001b[0m in \u001b[0;36mupdate_network\u001b[0;34m(self, target, gamma)\u001b[0m\n\u001b[1;32m    311\u001b[0m         \u001b[0mlamb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0.1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    312\u001b[0m         \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mbeta\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnext_state_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mbeta\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0maction_loss\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mlamb\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mq_loss\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 313\u001b[0;31m         \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    314\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    315\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature_optimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.9/site-packages/torch/_tensor.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    305\u001b[0m                 \u001b[0mcreate_graph\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    306\u001b[0m                 inputs=inputs)\n\u001b[0;32m--> 307\u001b[0;31m         \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mautograd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgradient\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    308\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    309\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mregister_hook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniforge3/envs/pytorch/lib/python3.9/site-packages/torch/autograd/__init__.py\u001b[0m in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    152\u001b[0m         \u001b[0mretain_graph\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    153\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 154\u001b[0;31m     Variable._execution_engine.run_backward(\n\u001b[0m\u001b[1;32m    155\u001b[0m         \u001b[0mtensors\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgrad_tensors_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mretain_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcreate_graph\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    156\u001b[0m         allow_unreachable=True, accumulate_grad=True)  # allow_unreachable flag\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "timesteps, rewards, trajectories, losses = agent.train(full_explore=1, warm_up=0, e_greedy=30, epsilon=0.8)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for y in range(agent.env.goal_y+1):\n",
    "#     for x in range(agent.env.goal_x+1):\n",
    "#         if agent.maze[y, x, 0] == 1:\n",
    "#             state = agent.get_state(y, x)\n",
    "#             action_values = agent.q_net(state.unsqueeze(0))[0]\n",
    "#             print(f\"Position: ({x},{y}) | Timestep Until Goal: {agent.last_visited_timestep[y, x] - torch.max(agent.last_visited_timestep)} | Action Values: {action_values.detach().numpy()}\")\n",
    "# print()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 360x360 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAABJEAAAE9CAYAAAC7qPOAAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjUuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/YYfK9AAAACXBIWXMAAAsTAAALEwEAmpwYAACAB0lEQVR4nO3dd3wUZf4H8M+TRui9t4AUQYoggmADK4p6nvrzbKd3ljvvrOedJ54NRey9K/beFaV3Aamh10CAQEKA9N6T5/fH7iabzZbZ3Zl9ZmY/79eLF8nu7Mw3m51vnvnOU4SUEkRERERERERERP7EqA6AiIiIiIiIiIjMj0UkIiIiIiIiIiIKiEUkIiIiIiIiIiIKiEUkIiIiIiIiIiIKiEUkIiIiIiIiIiIKiEUkIiIiIiIiIiIKKE51AMHq1KmTTEpKUh0GEYVp48aNOVLKzqrjCBVzEZE9MBcRkRlYPRcBzEdEdqAlF1muiJSUlITk5GTVYRBRmIQQh1THEA7mIiJ7YC4iIjOwei4CmI+I7EBLLjJsOJsQ4kMhRJYQYoeP54UQ4jUhRKoQYpsQYrRRsRBR9GIuIiKzYD4iIjNgLiKicBg5J9LHACb7ef4iAAOd//4G4G0DYyGi6PUxmIuIyBw+BvMREan3MZiLiChEhhWRpJQrAOT52eQPAD6VDmsBtBNCdDcqHiKKTsxFRGQWzEdEZAbMRUQUDpWrs/UEkO72fYbzMSKiSGIuIiKzYD4iIjNgLiIin1QWkYSXx6TXDYX4mxAiWQiRnJ2dbXBYRBRlmIuIyCw05SPmIiIyGNtGROSTyiJSBoDebt/3ApDpbUMp5XtSyjFSyjGdO1t65UsiMh/mIiIyC035iLmIiAzGthER+aSyiPQLgBuds/+fBqBQSnlUYTxEFJ2Yi4jILJiPiMgMmIuIyKc4o3YshPgKwEQAnYQQGQAeAxAPAFLKdwDMBXAxgFQAZQD+alQsRBS9mIuIyCyYj4jIDJiLiCgchhWRpJTXBnheArjDqOMTEQHMRURkHsxHRGQGzEVEFA6Vw9lM72hhOfYeL1YdBhFFuYKyKmxNL1AdBhFFuYrqWqw9kKs6DCKKclJKrNyXDUeti4gijUUkP8Y/vRQXvLxCdRhEFOWueW8t/vDm76rDIKIo9/ivO3HNe2uRmlWiOhQiimLfbEjHnz9Yj582H1EdClFUYhGJiMjk9hxjj0giUs+ViwrLqxVHQkTRLD2/DACQWVCuOBKi6MQiEhERERERERERBcQiEhERERERERERBcQiEhERERERERERBcQiEhERERERERERBcQiEhERERERERERBcQiEhERERERERERBcQiEhERERERERERBcQiEhEREREFJKXqCIiImIuIVGMRiYiIiIg0E0J1BEREgGAyIlKCRSQiIiIiIiIiIgqIRSQiIiIi0oxDSYjIDCSTEZESLCIRERERUUAcOUJEZsBcRKQWi0hERERERERERBQQi0hERERERERERBQQi0hERERERERERBQQi0hERERERERERBQQi0hERERERERERBQQi0hEREREFBBX0yYiM2AuIlKLRSQiIiIi0ozLaxORGQgmIyIlbFtESk7Lw0e/H1QdBhFFuZRjxfhg1UGUVtaoDoWIotiRgnJ8sOogjhVWqA6FiKJYUUU1Plh1EPuOF6sOhYhCZNsi0tI9WXhq7m7VYRBRlNt0OB/TZ+9CcQWLSESkTlpOKabP3oXDeWWqQyGiKFZQWo3ps3dhW0ah6lCIKES2LSIBHC9LROYhwYREROpJHRpHbF8RUbj0SCN65DMiCp5ti0hC6JOciIjC4Rqtz3YOEalUn4vC2QenHyGiMLnySDgFIOYiIrXsW0SCYHWaiJSrbyypDYOIoh0vuojIRNguIrIu+xaR2FgiIhMQvHIjIhPh/TUiUonXaETWZ9siEsAKNxGZB3tGEpFKroI252cjIlNgKiKyLNsWkQR4t42ITKB+7L/aMIgouulx9595jIjCJUT4BW3mIiK1bFtEYl9JIjIDZiIiMhUdLr7YxCKiUOmZPgSTEZES9i0iEREREREAfVZnIyLSC3sTEVmXbYtIDctqM0MRkTr13baZiohIId6xJyIz4Kq1RNZn3yIS20pEZAJMRURkJixoE5FKXLWWyPpsW0RyYWOJiMyAKyIRkUoNd/+Zi4hIPV6jEVmXbYtIDUvZEhGpI7g6GxGZQMMwf6VhEFGUY0GbyPpsW0TafbQIAHC0sFxxJEQUzfJKqwAAW9IL1AZCRFGtps5xwTZ7W2bY+2IhiohC5Spov7Vsf9j74ty3RGrYtog0f+cxAMCylGzFkRBRNFu9PxcA8MGqg4ojIaJoVlReDQD4Njkj5H1wvkkiCpszjxwpCP1GP3MRkVq2LSIREZkJu20TkUpcnY2IiIj0YP8ikoJujjuOFGLHkcKIH5eIzEflPCRpOaVYfzAv8gcmIlsKNY9lF1di6Z7j+gZDRJakx+psoeai8qpa/Lo1/CG9RNEuTnUAdnTJ66sAAGnPTFEcCRGppvLm/8QXlgNgLiIifQWb166buRb7skqwb8ZFiI+1//1LIvJNz3ZRsD0sp/2yE98kp6NHu0Sc0reDfoEQRRnb/yXnABIiMgPO/UhEKqkczHYwp1Th0YmIHDKdCy6VVNYqjoTI2gwtIgkhJgshUoQQqUKIqV6ebyuE+FUIsVUIsVMI8Ve9Y+CFGxGpzUWch4SIHFTmIk6JREQuSnORXjsiImUMKyIJIWIBvAngIgBDAVwrhBjqsdkdAHZJKUcCmAjgRSFEglExEVH0YS4iIjNgLiIiM1CdizjJP5H1GdkTaSyAVCnlASllFYCvAfzBYxsJoLVwZJNWAPIA1OgZhGRXJKJoZ45cpOfOiMiKlOYiXrcRkZMp2kVEZF1GFpF6Akh3+z7D+Zi7NwAMAZAJYDuAe6SUdQbGRETRR2kucl24saBNFPXYLiIiM1DbLtJjJ2Fim4woPEYWkbzlCM8z9kIAWwD0AHAygDeEEG2a7EiIvwkhkoUQydnZ2XrHSUT2pjQXmaGxRESmEPXtIl63EZmCbrkICD4fqewVyaF0RPowsoiUAaC32/e94Khmu/srgB+lQyqAgwBO9NyRlPI9KeUYKeWYzp07BxXEjLm7UVcXfKslUIU6r7QKRwrK/W5zvKgi6OMSke6U5iJXe2XPsWKUVenfE7yksibgykfpeWW6H5eIgqY4FzVcPB0t9N9+8aXWT3uqqqYOKceKfRzb8f/+7JKQjktEutItFwHhXaftyiwKansXf7lISokdRwr9vt5XriIibYwsIm0AMFAI0c85Eds1AH7x2OYwgHMBQAjRFcBgAAf0DKK6VmLj4fygX/fuCv9hjHlyEU5/ZqnfbcY9tSTo4xKR7pTmIuF2w++DlQeDfv2yPVl+n7/+/XWY9MJyv9uc+dyyoI9LRLozRbsIAO77ZmvQrzmYU4rtfi7Mps/ehQtfWYGM/KZFa9d9uYteXYmsYt5gI1LMNO2iKa+vDPr1VTV1fq/Tvt+YgUteX4WFO4/53ObpeXuwal9O0McmIoc4o3YspawRQtwJYAGAWAAfSil3CiFudz7/DoDpAD4WQmyHo2vlA1JK3c/omtrgeyJ9l5zu9/kQOjcRkQKqc5F7z+nq2uCnE1gaoIi0Nb0g6H0SUeSpzkXuKmtqg37NnqP+ewxsPOS4YVdQVo1e7X1vV1xRgy6tgz48EelEeS5yaxeFMsS1IkD+2pfl6PHorZe2+0iTg7mlOGNgp+ADICLjikgAIKWcC2Cux2PvuH2dCeACI2MA/Hd5JCL7U5mL3ItINcxFRFHNNO0ipiKiqGaWdpFKtSHc2CMiByOHs5lGTV3wSUJrZTzQXCSZAeZNIiJ7c++2bWRBO9D4/9/2WmfyXSIygFv6qQ2hXaR19+sO5vnd7r3fdB+dR0RUz9XbaEmAntzTft0ViXCIbCkqikihXLYdCFAccpn0wnIkTZ2DQ7net9cyuXZJZQ1O+N9cLNl9PKgYicgCwuyJJDVmsEteX4WkqXN8jvHfnlGgaT9jZyzGh6uCn7uJiKwjv7Ra933udg53mz57F5KmzsEnq9O8brdyn7aC9v+9sxrTftmpV3hEZBJGd0RKy3XMy7b+YB4G/G8uHp21o+HYIXSD+ve3W/HXj9brFh+RHURHESnIAbf+JmLzZVVqDmpC7Ba5P6sEtXUSry7ZF9Lrici83JsrZVXBzUOSnJaHz9ceDuo1X284jKqa0HsZZBVX4onZvDtHZGeBVpf1tD+7BP/4YlNQr3ltyb76uZfcr9u0XsRtSMvHxz4KUURkXaEUclzyS6tw2yfJfrdZtKvhpnxNncSnaw6hojr4eeBcftiUgWUp7M1N5C5KikjBbf+3zzZ6fXzsjMX41zdbvD730E87cMnrq4KMjIjszr2xFGy76ap31nh9/Jr31uAPb3jPN7O3HcWgh+cFdyAisj2tvRq9ufED73fhH/55O4Y9tsDrc7mlVRj88PyQj0lE5GnG3N1eh8x+uOogkqbO8bmAyYmPzEdpZY3R4RFFDRaRgpBVXImfNh/x+fyeY8VNHgun2k5E1ueeAfTKRWsP5GFrhv85kJrEwVxERCHyNbfk52sPoyTICzOmIqLoFk4K8DW35MuL9gIAyv30OCquYBGJSC/RUUTSYR/zdwQ/xI2IyP2CqU6HibU3HvI/aS0RUST4mgvSG70K6ERkfXoXkgvLq1HMXkZEERUdRSQdWi+3f94wxE3LZNmhYCOLyH7c20q1OpzkV77dMMQt2B4ARBS99G5jPPDDtvqvg1l5kj2RiEhPvibx9yQhDZ/UmyhaREcRSef9XfHWas3baklWbFAR2Zf7MLI6na/iHvxxu677IyLyRnhpzbg/9sW6Q2Hti4iiRzg5wNsr3R/bccT/UH/eryfSR3QUkXTOGMGuauKPlBKfr9Xe+CIiazFiTiSXozrmIgCYtcX3nG9EZG165h/PC7nckir/2wd5zbhsT1ZwLyAiAlBaGfoqbN4kp3EKASJvoqKIZOa6c/KhfHybnBGRY/28+QjS88oiciwicnKfE8nEY1allLjn6y0ROdbSPcexMzO4icGJKDx6Zh8JY3tR//XjDcbt3E1yWh7WHsiNyLGIyBhG5iJfq+TqLeVYMRbu5Py7ZB1xqgOIBB3msjVMhdsqAuEsv6vFvd9sQceWCdj4yPmGHoeIGrh32w5m3hA7u/njZABA2jNTFEdCRKGyw1B81wUicxGRdWldfdbE9/Fw4SsrADAXkXVERU+kcJOGnnep/vTuGiRNnRPy67PCnNQ7t9R/l3Mi0pd72ybcXLTOIxeFs7vHZu0ILxcVV+iy2hwRmV9lTeMhIiv35jQawhZMJvC8YfbFukNImjoHuSWVIcWWV1qFqpq6kF5LRNY2d/tRZORrH9rvr9y0/mAekqbOwZb0gpBiKSyvbtQ5gMjOoqOIFGYPn2veW6tTJMC6g43H1rpfVAaaaG7ZniyMfWoJ5wogshD3szrc4Wx/0jEXfbKm8VxswYSWnleGsTOW4O3f9usWDxEZK5yVaj3nGXl58V7sOVYcbkgAgG82pANAUBeC7kZPX4R7v9msSyxEZDw9R17szCzCV+sPN+w7jDy3LMVxffV7ak5Irx/5+ML6HkVEdmfbItKVo3vVf63yZnkw3b0lJD5bk4aCMu+9hTY7K+NbMwrCD4yIIuLcIV3qv1Y5J1KwQ09mbfE9h5prcYHf9maHGxYRRciJ3doYt3MDc9vSPccDrrg0dzvnEiGyioRYa15+JqflYc1+/6NTDuVy7lmKDtY8izW47ax+9V+H2kU6Etwv7HYcKcIjs3bi/u+3qQuIiHR11qDO9V+n5+m7mlo4d9wCuefrLbjsjVWG7Z+IIqtPxxahv1jh/Ec3f5yMS15nLiKyizgDi0hG3qq76p01uHamfj3CiazMtkWkGLfqzOO/7lIYSfB89UQiIutxz0W7jhYpiyPQcFlv8suqDYiEiOymNoiCdii5iIhIi9SsEtUhEEUF2xaRzNJECdRY8tbuMvPqAURkT0w7RBSqcKYNYJuHiPSSWeC7x3egVMNcRKSdfYtIHrWb6trIrNyx97g+E03qzchhL0Tkm2cuKoxQ756le45H5DhEZE1pOaW67cvf7bJPVqfpdhwisp/Nh/O1bxygl4C/+R9nzNE2MiXYOSSJopGNi0iNM8CvWzMjctwv1x0OvJEbb4kq+VA+dmb6n0SSiKzBszfikxobMVr4Kw0/PXdP4zhCbBRx8mwie7rqnTWatw3nmuqxX3b637fGnf+4KSOMKIjIrP741mrd9lXrp1uk1gn4A913f28FV6Ylsm8RyeP7/dnmHCPrK1Hd/PGGyAZCRIaI8UhG320054WQr96KN324PsKREFEk5Oi46Mi2DONvfN337VbDj0FE1vbV+vSQX6u1oP2Ux006omhk3yKSRyYor4rMcDatKmtq/T6fX6rvkBeOZiNSwzMX6Wnz4YKw91FR7T8X+cW8QkQAVqXm+H3evUem9JE4yqoC56K6cCZfIiLSoKyqJuA2kZomhcisbFtE8rz7bzaDH56P7OJKfLHukNfnq2rrfPaeYkGIyDpMnopw4iPzUVFdi/dWHvC5zYKdjbuAm/1nIiJz8VU4AhraNNfOXIuiimq87ycXvbpkn96hEREBaMhFby7bj52Zhfh2g+9eTf/4fFOEoiIyJ9sWkTznIamLUOXFs3rtrxPCP7/YiAU7fU9+eyhXv0kviUgNVRM0BjNU5bUl+/Dc/BSfz284mNfoe9axiaJLpPLYr1sz8eSc3T6fX7SLCwYQUei05rL1B/Pw3x+2+Xx+8W7mIopu9i0ieSSJmrrIdDv8Nln7fCdbg5hD4NetmXjNeQeOqwYQWYeRw9n8yQ9iFbhgJs/ecaQQ17y31vENcxER6ejnzUc0b1tQVoWkqXMMjIaIzMazk4BRPvo9LajtmYso2kRNEcmMQ8CqarQXtsJdlcSEPz4RRZC/ZtfOzCLN+1mzPzf8YIiInNzbaxvStC/1nZFfbkA0RETA4bwy1SEQmZqNi0iNL5lO699RUSQNjhQE1+AxY+GLiMLzlwlJqkNAZrC5yKA4iMgajLr7n1VcEdQNNeYiIgqHr2urvNKq8BYaIYoycaoDMIpncydScyL5MntbJu78crMu+wr2ApCIzKNFQqzS46dmleC8l37TZV9H2BOAyLLiYyM3HrW6tmkbrKa2DmNnLNHtGLV1ErFmX1WFiJTadNh7b8fR0xfpdoyK6lokxqtt6xEZzbY9kWI8eiK9uHBvk21ySipxrLAiIvFs0WEpbpdvkzNwwMfKbb5IdmsiMoW3lu9v8lhxRTUO5xrbddqVEtPz9TvOkYJyzN9xVLf9EVHkeCvsVNbUYt/x4ogcv1bndsnLi5q284jIGjyvU6SU2Jmpfe5YrYoragJvFKbbP99o+DGIVLNtEclzTiRvY1vHPLkYpz3d+C7YXV/p01tID/7aV0cjVPwiIuNd8dZqnPX8skaPPTXX9wpFkeYvF20/on8jj4jUePCH7Tj/5RXIL62qf+zDVQdRbpJhHv5uiK1MzYlgJESkJ8+i9ser0zDltVWN5mGct/0ofghzjthIWJ6ifbESIquybxEpxNf9ujVT1zjCUVOn/S5dcUU1sopZWCKyon1ZTXsWvrfigIJIvCuv1n7nrqK6lkNuiSxq3cE8AEBpVcM5/8TsXarCaaKo3Peqk3Uebaa6OolDuaVGh0REOvBcRdu14Id77+l/fLEpojHp6WAOcxHZi32LSF6W1TaiW6SRnp6nvSfCeS/9puvcAkRknNX7rXXH/Kv16T6f86x13/nlJkx4ZqnBERGRHn7efER1CEHJ9NML27NX5KtL9uHs55cHPfyfiCLPzsNRF+06jkkvLMe87Rz+T/Zh4yJS08emvLaq/uvaIHr56BFHKEc75DZHSqDXHy+q9Pu8lWZE+i45HceL2KuK7Ou6mevqv47UfGX1hwnzcNJjB56pdvHurPAOYCILdh5DalZk5ochUuHeb7Y0+r6k0vj5Qly5yOjUt+6gYxjMMRu0J1bvz8HGQ94nBCayg5krDyLPbRhtaQRyUaTsPuroVbXL+b+V7ThSiGUp9mnnUejsW0QK8PwDP2yLSBxF5TUcZhaE/NIq3P/9Ntz04XrVoRBFxIe/p0XkOOXVtTjCYWZB+ftnG3HeSytUh0FkKFche3VqDgr9DBfTS22dRBqHdgTlupnrcOXbq1WHQWSoMucw2qOF5Zi345jjQYOLzalephMg3y55fRX++tEG1WGQCdi2iOS5OptLhXNyyO83RmZitmtnrtVlmFm0LFrrmgcqp8R/zyoiq9ue4Rh6EanVzV5ZvA+n6zDMTHhkIyv1ciSipr7e4Biuuj4tLyLHO1pYgYkvLOcCIUTUyCM/7wAApOdF7obXeS/9hmV72LOGKFi2LSL5qCHhxEfmRzYQpw9WHdR1f8F2A4/QiBki0ujSN1ZFbCibuy/XH474MYnIvB78cXtEhrF5+maD77nWiCj6LEvJRsqxyA8h/3oD20VEwbJvEclP351dmdYZk7r5cHSOgWfRi6LBj5uO+M1VRli063hIr+OdOiL7euD7bRHPRe/8tj+k12mdDJztCCLrufCVFT47Ahhlwc7Q2kUf/66tgwBzEdmRfYtIfn4y9+UiXSqqa5X0Cgjkj295HwP/5rJU3PZpcoSjMV6k/3AQqbQ1o6DJWNXKmtqITfwfjL9+7H0M/C9bMnHxqysjHA0R6Wnlvuwmf3+raupQU1vn/QUKeU4G7m7C00ua9KqKdHGMiMLjecbW1NahqsZ8uWjar7t85shTpi9Cel7j601mIrKTONUBGMXfieo5X1JuSSVOeXIx/n5Wf2OD0tGaA7mqQyCiMHmbu23ww/Mx+aRuCqIJzZGCck7YTWRxMTFNc9Ggh+dhYJdWCqIJXWZhBXZlFmFsvw6qQyEinUx6cXlE50kKhvBx9zu3tAo/bMrAvecNinBERJFh355Ifrq0eD5zvMgxifO7Kw4YGFF4wu2X4LksNxGpJwSw/mDTyWzn7zymIBoiilYxQmD1/pwmj+/jykVEZKCzB3Vu8tje443zjlkLSETRzLZFJC831Rqes+1Prc3lb/6OQ7nmXl6XJS+KBtE+zOLPH6zzWkQjImO1SWzcET1GAGsPRO+5+K9vtuCXrZmqwyCKOj3aJTZ57H8/bVcQiTk8M28PZpq4UwORi23LKf4uzjyfi7Z5eLakF+DNZamqw/Aqyn4VRJajZ6/Glfty8C8/85sQkTES4mJVh2C4YDLVT5uP4O6vNhsWCxF51ywqcpH2bPTOb/sxY+5uA6Mh0od9i0h+qhH3fB19DQUTzhlOFPU+1LiyBxGRnvp3btno+5ySKkWR6M9zkZRou1FIZCU92zX3+7yVp+Nocu3FZEQ2YmgRSQgxWQiRIoRIFUJM9bHNRCHEFiHETiHEb/odu+HrLq2bNXquqKIGVhNtaceMK+WRdanMRe6mXnSiEbuNqGgfgkcUDrPkoif+cJIRuyUiizBLLvrT2N5G7NaceG1DNmLY6mxCiFgAbwI4H0AGgA1CiF+klLvctmkH4C0Ak6WUh4UQXXQ7vtuFztAebZCVku0nVr2OSuHyNyE6UShU5yJ3ngVtO5NS8nwmcmOmXNQywbaL8zbB6zaixsyUi+KjYKJa3nwjOzLyzB0LIFVKeUBKWQXgawB/8NjmOgA/SikPA4CUMkuvg7tfu7ROjPe77ZLduh2WiMxHaS5yF+tvxn8Au48WGXFYIjIHy+SivFLrD2/jZRuRT6bJRYHuNdXWGXFUIgqXkUWkngDS3b7PcD7mbhCA9kKI5UKIjUKIG/U6eIxbVmqZ4H/StucXpOh1WMNEy400DmMjAyjNRe5iArSWPlzFOZKIbMw0uShQEWmxDW6usTVB5JNpclEgRwrKVBxWV1ae14nIFyOLSN5aKJ5nURyAUwBMAXAhgEeEEIOa7EiIvwkhkoUQydnZvoel+Tp4TIDGktld/OpK1SFocvmbv2PEtAWqwyDypDQXuQtURPpuY0bQ+4ykMU8uQmVNreowAnrg+21ImjpHdRhEnkyTiwIVkV5bsi/ofUbSiY/Mw/GiCk3bqhxV+/7KA0iaOgclldabi5NsTbdcBISXjwK1i95ctj+o/UXa0EfnY0t6gbaNFSaj3/ZmI2nqHOw9XqwsBrIXI4tIGQDcZ0vrBSDTyzbzpZSlUsocACsAjPTckZTyPSnlGCnlmM6dO2s6uPt5GmfxItKuAENcSkNonBjR4WdLekHYk5ZzDhUygNJc5C7W4kP/c0qqcEzjhZtWRvQ+/CY5PfBGRJFnmlxk9XZRRXUdluzx3lvKlVHWH8yLXEA+fLrmEAAgt6RScSREjeiWi4Dw8pHVm/2VNXWYufKA1+dcueiVxeqL8vO2HwUAbDyUrzgSsgsjL2k2ABgohOgnhEgAcA2AXzy2mQXgTCFEnBCiBYBxAHbrcXD3YoTF81NAWi7qOEqMopjSXNSY9bMRcwlRyEyTi6zeQ5uIwmKaXMRMRGRNhi3PIaWsEULcCWABgFgAH0opdwohbnc+/46UcrcQYj6AbQDqALwvpdxhVExWtvZAruoQIorXqaQX5iJ9rdyXozoEIktiLtLX2v3e20W8KCXyz0y5KNBwNivYqnU4G5GNGLrGq5RyLoC5Ho+94/H98wCeNzKO8Sd0xCfOLsVWVVEd2vIEF726EjkllVhx/ySdIzKG9f+UkBmZJRcN6NLSyN1HxOG80Ca5vPfrzfh5SybSnpmic0RE1mGWXNQszuJjawEcyCkN6XVvLkvF8wtSsGf6ZJ0jIrIOs+QiG9SQkJFfHtLrFu86jls/TcbSf5+tc0RExrN+K0KDycO6Y/ZdZ6gOwzD+8u/uo0XILq7kygBEJjCgS2usesAaBV29/bzFc7oFIlKlWVwstj52geowIsKzjfTR745VMIvDnMORiMInhMCOxy9UHYYSv25ztIu2ZRRG7JickoD0EhVFJAAY1rMtbjuzn+owiCjK9WrfAl/eOk51GLozYoJsIjJO2+bx2PzI+arD0J0ZMxHTI5FvrZrFYe+TF6kOQ38eJ77KTld26PFF5hI1RSQAGNGrneoQSCM2uMjO+nW2/rA2IrK+9i0TVIegDHtoE5lHgg2G2HrSmmEikYt4XUV6s98Z60edTc8gYaPyso1+FCKfBGf/qmen/EVE5A3THBGZ4SqUuYj0ElVFpGhm0/oZkSXxj3gDDoMjIiP5KlSzmE9EZsBcRFYUVUUku/ZEsiNeWBIREZFROJyNiMyAuYisKKqKSKxLmB+r8RQN7JiLPH8mO/6MRGR+TXORZzJiO4OIjOeZejwzj4pMxLYZ6SWqikh1UXziRPGPTmQ6vOtERKRK5PMvL9yIok+gtl4k0wKnUSC9RVURya5DpOyUF4y+uP7LR+vxzy82GnoMokBsmoooCNN+2Ykpr61UHQYRRbkPVx3EsMcWqA6DiKLcsj1ZSJo6B4Vl1apDIQ00FZGEEP8nhGjt/PphIcSPQojRxoYWvj4dWuD+CwfXf8/rNlqeko2524+pDoNCZNVcdObATrh6TK/67zk/G328Og07M4tUh0EhsmouuuG0PhjXr4PqMCKq6cTakb/1ZuZeAE/M3oWSyhrVYVAYrJiP/nfxiejeNlF1GEpFMi1Yodn51vJUAEDK8WLFkZAWcRq3e0RK+Z0Q4gwAFwJ4AcDbAMYZFpkOVvx3UqPvWzXT+uNax/cbMzD1h21Bv86suYRzIlEAlsxFn93SOLz4WPt1Al13MA93frlJdRhEkWLJXPTk5cNVh2C49Lwy3PZpsuowiCLJcvnob2edgL+ddYLqMAy1LaMQp0xfpDqMRsxc0CZr0XolU+v8fwqAt6WUswAkGBOScS4a1k11CLr7z3dbUeM22dOz8/fghvfXNdnOyKF8xworkDR1jmH7J3Jji1zUtY397r69uDAFuaVV9d9/teEwznpuWcDX6ZmZqmvrmIsoUmyRi+zox80ZjXrWrErNwfDHFkS8t82YJxfhcF5ZRI9JUYv5yIQ2pOU1ahel5ZZi0EPzkJZTCiByN/T/753V+HpDuuOYZu1FQJajtYh0RAjxLoCrAcwVQjQL4rWm0bRLs/28vXw/VqXmRPSYmw/n675P5jjywRa5KBo89NOOiF9AlVXWBt6ISB/MRRbx2pJ9KK6sQcqxyA6RyCmpCrwRkT5skY9G9WmnOgRDzdqSiaraOvyyNTOix92Qpv91GpHWBHM1gAUAJkspCwB0AHC/UUFRFLN/nY/Cw1xERGbAXESa8e4/GcwW+SjazhNe8pCVaSoiSSnLAGQBOMP5UA2AfUYFReFbsTcb5VUNd+WjLC+TTTEXmVfyIe93upLT8pBbUhnhaIiMxVxkXmsP5Hl9fNfRIqS7945kw4hswi75yG6raFdU13l9PC2nFHs5eTRZnKaZpoUQjwEYA2AwgI8AxAP4HMDpxoVG4bjxw/W4/OQeqsMg0hVzkfVc9c4aJHVsoToMIl3ZKRe1bR6PwnL7L6n8yM87AACdWjVTHAmRvuySjzq3TgRQqDoMw/24+Qh+3HyE12lkaVqHs/0RwGUASgFASpkJoLVRQRnp27+PVx1CxBxwTtxmNEOmmrLXzQjSj21y0bx7zlQdQsSk5fqeG0nX9MG+4RQ5tslFs+86I/BGdhTBfBEFU3KSWrbIRy/+30jVIRCRRlqLSFXS0cdQAoAQoqVxIRlrbL8OqkOImG0Zvqv5Zu0xqrqhVVxRjYpqTs5rYrbJRUO6t1EdAplYWVUNyqoiu5oUBcU2uah3h+jqKZjjGl5r0naQ2VTW1KKowv491SzOFvmobYt4TBneXXUYEbMlvUB1CKYS6Nq0prYOBWVcsMAstBaRvnXO+t9OCHEbgMUAZhoXFunN14m5al8O5u84GtlgTGz4tIW45PVVqsMg35iLbGrHkUJ8tf6w6jBMY/i0hRj66ALVYZBvtspFLRNiVYdgGhn5ZXh7+X7VYZjGVW+vwYhpC1WHQf7ZJh/1aJeoOoSIcfXS9nWNVlxRjRcWpKCm1vvcSnblq0PBQz/twMlPLEJVTXS9H2aldWLtFwB8D+AHOMbbPiqlfN3IwIzE8fANJ+gNH6zD7Z9vCndvYcfjSeUNwtSsEoVH189ve7Mjvoyo0eyWi64d21t1CMq5zvVLXl+FB3/cHta+VPdk1FNtnX26SWzLKMCna9JUh6Eru+Wi168bpToE07j54w14dv4eHCko122fZu39rcX2I/aZoyY9rwyvLt5nuwmc7ZSP7pg0QHUIpvHs/D14Y1kqft1mr7Z8qFzXNDV11i8iFVVU46m5uy1dENNURHJ2i1wqpbwfjsp2cyFEvKGRGWhAF0v28owKNroGVO6mD9fj7q82G7b/1KxivLQwJaKNMbvlou5tm6sOgchwl73xOx6dtdOw/eeUVGLaLztRHcG7tXbLRW0SLRu67korHUPa7VZoIOCWTzbg5cV7kZ6nX4HQXWVNLR6dtQOFZZEd/menfJQQp3WQjH34ugHmWt2tupa5CACkjcYgv7ggBe+tOIAfN2UYsn8pJZ6dvwdpBs6PrPVMXQGgmRCiJxxdJP8K4GOjgjKaiMZShX3OOzKJa95bh9eWpqIgso0lm+UiIgrX47/uwser07Bo1/FIHtZeuYjJiKKA66LcqIvRHzcdwadrDuG5BXsM2b8ftslHMVGYjFivDo4druOrnIXBWoN++YdyHcOyb/lkgyH7B7QXkYSUsgzAFQBel1L+EcBQw6IyWBTmJ01mbTmCHYq7LUdTHpVSorSyYeLc7OJKhdEET1F3UuaiKLAsJQurU3NUhxFVPHORlXph1DpzUYRDtlUuYknbu02H8zF3e/jzRjLXa1daWVOff/JLqyw1H0ydM24Fo5Ftk494rnh3ILuE80Y6RapHUnlVLeqcJ3NhubUWXnK9Q0ZOjaC5iCSEGA/gegBznI/FGROS8ZigvLvn6y2cVDqC3v5tP056bAGyiysxd/tRnDpjMdbsz1UdltnZLBcxGXnz14824Lr316kOI2os25OFkx5bgOS0PKRmFePUGYvx8eo01WGZnc1ykeoIzOmKt1bjn1+EO28kaXU4twwnPbYAX64/jJraOoyavggP/BDeXHlRwjb5KBp7Imn5kfWYN9IqzHAPq7ZOYsij8/HoLzsAACMfX4g/vrVacVTmorWIdC+ABwH8JKXcKYToD2CZYVEZzA7d4MKl5wm662iRfjuLIrO3Ou5uHi+qQHJaPgBgZ6b3nmCr9uXg87WHIhabid0LG+Ui0tdh50onFJzfnb2+Nh8uQFqO4z1ctc97T7BdmUV4fcm+iMVmYvfCRrmIraKmwmkn5ZZYq2exWezPcSxssnDncdQ476DP9jGp8LHCCkyfvSukO+1muEjV2b2wST6KxiJSwM+jBMqqQusFY6XeM1p/9ZG4jneNtvh2Q8OcRbt9XO+WVdXg0Vk7UOLWo1srK+ciTVVqKeVvAH4DACFEDIAcKeXdRgZmpCjMT4Z2/XvNywVFVlEFjhSUY1Sf9oYdN5rc8IGjV8YNp/VVHElTkcx/zEXkj7eelCWVNdiWXoAJAzopiMh+LntjFWrqJO46d6DqUJqI5KSb9stFTEZ6uvmT5EbfSwnU1Nbht73ZOOfELny/dXD/91uxcl8OzhvSFeNP6KjpNZF72yN7ZWinfBTDU6OeHm/FCwtSvD6+dM9xnDmwM+JjrTORuVkLLh+vTsOnaw6hbfN4/PuCwZpeE6lcZORbpnV1ti+FEG2cs//vApAihLjfwLjI4i54ZYXPbn9b0gtw5nNLUVzhe0JmKSXq6qSl5uWINir+ztstF7FXpPGNgru/2ozr3l+HrOKKJs8dLSzHaU8twaFc/6tXMBc1qFEw2UcgKs4j++Wi6GPkJ/lIftNekY5JTpOxdE9Wk+cqqmsx6YXlWHvA/5D2ujpZP0dHtHOtxmimFZtU/U23Uz5igVVf2V56Ra7cl42bP07Gq4u99yq+6u3V+GGj/1XDXNdpetLazDLbR8T1PtSZqJ0YibdIa/lxqJSyCMDlAOYC6APgz0YFZTQmKOP5W7HrxYUpSM8rx6bDBT63qamT6P+/uXhmfsRXuCBzs1kuUh2B/e09XgwAqKxuOjnrz5szcayoAl8GmKyy///m4i8fGbfChRWxqMZcRMFJdxaWcrxc1O07XoKDOaV4cs4uv/sY/eQiTHhmqSHxWVbUpyIANstHZBwJidySKgANOclT8qF8/Pu7rX73c93Mdej/v7m6xwdY9+9RtDWLtBaR4oUQ8XAkp1lSympYOG27PpvN4qzThS9cen2wV+7LRmFZNX7efETT6kla5iiprZMo8uiVVOW8w/Tpas4DRI3YMhdR8DYeysPRwnIsT8nCnG2BV0/SMneblBKFPgrgv+3NDjpGq3G/o2/ZkypybJaLGrLRiF5tFUYSOXrl391Hi5CaVYIt6QX4Yp33Nov7RZFrDsRAvOWigrJqHCtq2qvSznydVOH0+rHsieqbrfKRyxWjeqoOwVIO55Zhe0YhDueW4Y2l+wLe7FmhsV1TWF7dpNfRmgC9Ju1CS0/HUDqn2KHgpLWK8i6ANAAtAawQQvQFYNnZlF2/67dvGI27zxmgNhgLWXsgF3/+YD3+9lky7v1mi6bVk856fhnKfUwG50pu02fvwohpCxtN/mb0yWWGJWPdc45Vq+5AxHsk2DIX3XpGP/zwj/Fqg7GQgzmluPLtNTj7+eX4y0cbcMeXvldPcr3Hf/9sI477uvhyfoS/TU7HyCcWIuVYsc4R+xbKRIx6CyUXmbEBFOGYbJmLhnZvg/dvGqM2mAjR4+NytLAcF726Eue99Bsuf/N3PPTTDp/buooe323M8DlBq+szvPlwPkY+sdDnpNJG8NZDSpVgfjcmTEUq2CofuTx1xXDVIVhGQVkVznp+GS59YxVu/HAdXli4F1nF3s9pV77PL6vGkt3H/e63sKwaIx9fiBcWep9byQhH8su9Ph7Jcz2UIrUZc5GR7SJNRSQp5WtSyp5SyoulwyEAk4wLy1iuj4WUwNh+3ifj+8fEEyIXkEVc895aAMD+7JKgXud5Z86zYvvzliMA4LXYVG7QqgLPRzAZ2pWKYaH2y0WO91ACGNi1tddt+nRoEcGIIi+Uj9GkF5YDAKpqgisGP/RT4+VxPY/t6m2UmhVcjgvHbR4T8FIIFBTh7ZaLXCSALq0TVYdhGeOfDm1o2VVvN54z0jMX7ch0XP+v2R+5u/1nP6d+Ma9gTuVwmiBGp4xIF9ntmo8S42NVh2AKWnrDXOmWU1zXTt4+h57FkVsCtEHyyhxD3+ZuD9zjWy/3frMlYsdSxehLqEhcommdWLutEOIlIUSy89+LcFS7LempK4bjT2N646xBnX1uM0HjSg9WpddEhC8t2hvya1XOMbI3gj0N7ErFnCh2y0XXjeuDK0f3wl3nDPC5rO2UEd0jHJU1fbDqYMBtfH1k311xQFmPoE2HtQ1tMRsz3nGLJLvloiHd2+CG0/rgretH+9wmWoa5heuHjRnIcc454uKZe3zNR7szsyiiRWx3pSEuIU7q2S0f3X3OAHx/O3tnB2N/dsMCIa58s3RPFpZ5mchfq6V7/PdSouimdTjbhwCKAVzt/FcE4COjgjJa97bN8exVIxAfG+OzUme3VZM82yvrD+ZhW0ZB2Pt9bYn3mf0pekS4R5KtclHLZnF48eqRaNciwWYZR7ujhRWa5lcLZPps7xPSuudyf4UPn0PdyPzUVLRslYtiYwSevHw4+nXyfd0ZDTlq/o5jYe/D14S07n8q/d3IW70//HxIainoqG2rfHTfBYMxJqmD6jCU+t5tdbRQr0n/99N2FFU0vkEWTCeCbzf4X6GNzCsS9/njNG53gpTySrfvHxdCbDEgHlO6eHg3zN0efsPCTDLyy3HZG78rOXY0NETJMLbNRb4L2g3G9+9oy8kMtcyvZgQz5CKz9ugJ1NPQ8bwZ3sEGEb5ws20u0qJt83gUlvtehdWqbv98I3q2a646DApCKBdLZs27YYjqfGR13j7D//luK0b1aRfxWCh0Zpwr0sh2kdaeSOVCiDNc3wghTgfgfdYri/H13nZqnVD/9Sl9o7sa3pQxn8hgzr0jBeV4aWFKyEOqPF/16Zo0pOcFXknOCGZMOsGI8LA2G+ci7+dV/86t6r/u2Z4XN0Zx/xgHc6eupLIGM+bsQmWNPkNBZm05gh1HCnXZV7C0nMpmXgQgwrnUtrnIlxG92tV/3SKBc5UEK5RJ64P5SNfWSTwzb49uE2QvS8nSpZdoKLS0K0LJRZFKXwradVGVj0ZGydDaympjFgHSOoIgnBVb31qeqtvQ3E2H8x1zMim4XjK6XWT0j2RkLtLaE+l2AJ8KIVxnbT6Am4wJSa1Nj5yP9LwynNitTf1jPdtZf5JJFfPXaOU695anZOHcIV01veYfn2/EtoxCTBnRA4O7eZ+QWKvCsmo8Omsn+nQIPKcKABwrrMDGQ/lRP1eNiom1YeNc5Pl2Lv332cgvq8boPu3wH+fwCN4hD15BWcPcJIHyoKuQ9/PmI7hkRA9N+39tyT7MXHkQvTu0wI3jk0KO0+Wer7do3raksgZztmXi6jG9wzofvb020P5M9RdFTWHLtrnI07d/H4/m8bEY3K01PlvrWChjQJdWOFpozyGgFQYs6CEEUFqpfQVa10f6y3WH8dQfta1QtWJvNt75bT/Sckrxzp9PCTHSBn8NYt7K2jqJr9Yfxp9O7Y34WK33p/Wh1xyfelBYZI+KfPTW9aMxoEsrdG+biOHTFgIAWjeLQ7EJVjkNh6/PTVmVMT9XMCtUu0I7lFuGOl+TuXkor6rFc/NTMHPFAWx+9ALNx/K19yveckwanuDMLYHy5/cbM3D+kK5o2yJe87E9hXIuB5OLjE4VpplYW0q5VUo5EsAIACOklKMAnGNoZJHifJOHdG+DPdMno0PLBIzs3a7RJhee1C3ycdmI58nu64N937fe5xHwxlWd16PxUOcMUGvX/P97dzXu+HITajUm00DMfGffbOyci9w/B7ueuBD9O7fCKX3bN7qYv+ucAQoiszbP+QDcNT73Gs7nxbu1T0TpWiGupjbyFzKPztqBB37Yjg1p4U3OHcxNBqYrBzvnInfbpl2Asf06YHivtkiIa2gyvnn9aNveSMktrQq8UQh+2Zrp87lG8yWFeNPP1SapDuICUS/fbEjHwz/vwMyVB3TZn5aiuN3mLg1HNOSjzY+cj4uHd8egrq3ROrGhOLD432fjnRvCL5qq5OuUT8t1jJDQu1D61Nw9mrbzjGtDWp621znjrTCoJ5U/KceK8Z/vtuLf320Jaz/BpOFQcpF5St+h09oTCQAgpSxy+/Y+AK/oGo0Crl98m8S4JktJ3n/hYJw9qLOqHhfkRzAJdceRQvTv3BItErx/3F2/Xq1FpCP5tu0hbBl2zkUAmnxWn75iOHq2a4642BiM7dcB6w9q+0NO5pKaVYyOLZuhfcuEwBtrkF3sGLZSrlPPiWD+1Jm4c2tE2TEXuWuT2PhO7js3nILKmlq0SYzHwC6tfLyKPAU6X/Q4n4LZRXpeGeJiBbq31ad3q6v9ZJV5sszcOz8cds5Hnn83v7t9PLZnFKJrm0R0bKXP31S78PfpFhAhD3mt1XjeBHN6ZRVXoLSy1u+iDi5VGgrkrp6kWcX6DOs1ugRg5QpDUEUkD1b+uTW5YxLv+ltdUUU1Lnl9Fc4b0hXv3zRG130bMbGsFds1JgjZFrnI3x+qa8f2qf/aro3fSFD9zp330gp0ad0M6x86T3EkgVnxc2aCiG2Ri/yZPKyhZ7ZOnXGVUfkRV/3WnfncMgBA2jNT9N2x6h+M3Nk6H52a1AGnOldws+Cfq6DomWvNNPwTAMbOWALAkYv0/MBa5TNh+JxIBh4hnIHLFvn1aGOrH8YLPX8+M3TM8uw6uD+7BL966SruGva2Jd33cI9guyHq3TMtr7QKGw+FNxwlGKtTc3RZxtgEHwMXW5y+MVonOrTFT+tbRr6aCe4B6PKhzi6uxBfrDvl8Xq+7Y+70KvhkF1diV2ZR4A11sjOzEN9sOBz2fpiLFLF5Mlq+Nztix/JM/3pMGl1RXYuZKw7oNvTe77F1PgmziipwILvU8Y0Bvbg823EZ+WV497f9we/IB5OcGuaIIgLqTPKGG0XPz2YkeOYDKSU+WHUQJRGYu0rvXFRdK5FZoP8IFF9hllTW4KWFKUHNW+V9/8a3jPz2RBJCFMN7EhIAbDHDa/2Hzd75x3Y8K6vnvvgbAODSkdomw21E0RWI67N344frg3qdlBI1dTLkyStdy6nrfgfSQFGRizRuZ/fG0jEDJ+sN+B7r8Nb+4/ONSD6UjzMHdEafji0CvyCMY+pV0HbtZ+ZKbYsLAA05uKqmrtFcOcGY8toqAMCfTu0TYEvziIZcpJXVeyIFOn22pRcYd+wAz+uxbP1rS/bhreX70a5FPP5vTO/gd6jQnmPFuPi1lX638fz9hZOLbv54A/YeL8GlI3ugRxgLWES6Ocl85GDzZlH93EhW4fn7WJ6SjemzdyHlWBGeu2pk+PuPwEW7e36Z8MxSbS9yhlVdW4e4GBFSG+25+Xvw6ZpD6NuxJa48pVfQr48kv9lWStlaStnGy7/WUspwhsKZhonuYFqGmZK11kprTkmVY3lINze8vw5TXlupuWpdVyfrJ9EFfF/73fzxBiRNnYPP1/rujRCOZ+bvwcCH5mleUlyPXgqHckuRNHUOzn5+GZKdE+tF8mMQFblI6/LPxoZhK553cvy9d+GeJq7fX55zUt7qOt93kd5antro+0dn7cCQR+YHdTz3899X6I/O2oGkqXPw7Hxtk2hq4d4o+mlzBgY9PA8Hc0o1vVaPVa/KqmqQNHUOTp2xuElOj4RoyEVamW1YRLAi2ZbxnCvIby4K81iuU7TYuaiAvznTHvh+W6PvP1mdhqSpczS3LwBtuWjmigNImjoHd3+12e++QrnoknDMfTno4XlYsvu4ptfklTbuEepaOS/YmzTDHluAkx6dj49/P1gfSyQxHzlYPRdF0v4sbX+vAf1yketvf1G5755IV7+7ptH3K/ZmI2nqnKB6pzfORd6jn7f9KJKmzsHlb/6ueb+BuKetwrJqDHxoHt7W2HssI6/xz+d6r2r8tCG9ueyNVUiaOgfPL9CvvReIoetwCiEmCyFShBCpQoipfrY7VQhRK4S4ysh4opnasf+ND67rmNcAKc79xH518b5Gz61KzcFOH0M3soorcMSj++KMubsx6OF5AbuGL93jWNnp4Z93YNaWI/WPJ6flaV4e0+VgTime87gA/HKtY/hHZY22BONrRZgNaXn4v3dW1yfp6to6FJZ5nxRzxb4cAI4lPoNZ9tcsrJCLtDaerX73P9wuusHIC7DKkp7dfT1zrL+c+9z8FPcg8OmaQz4v9ArLq3Egu6TRY98lp2Pww/NxONd/Y/DTNY5C9tvL9+PTNWn1j2/PKER5VXAFnYKyKjw2a0d9/pMSmLfdMSw25Vixpn08PXe318d/2ZqJGXN2IWnqHGw6nI+6Ounzd+daVj67uBL//GJTo+esMI+TFXKRVlbPRSWVkZsEenmK/6FxqnLRN8npjb5/dYmjnVTiZVXLiupa7D7auM20Nb0Agx+ej+Up/le0nOE893/ZmonXl+yrP1f3HS9GfpCr4VXX1mHaLzsb5YhNhx1TAgR6n12rRd34gfce4O+vPIgfN2Ugaeqc+mHJuX4mIS6prEFpVS2m/bqr0eNmmPYhEDvlIgukfr8yIrhgz4e/a+9xHK4mucjPNZvngjGu3LT5cEGTbeukI/e4yy+twuCH5+Pd3/yvEPnIrB0AgC3pBXhxYUp90eZwbhmOFgb3e5BS4sWFKdif5WijSQDZJY42yg8bMzTt490V3uOdPns3Nh/OR9LUObjv2y0AHLnIVztnW0YhAODNZY2LV0aeG4YVkYQQsQDeBHARgKEArhVCDPWx3bMAFhgViz+urq+tE/0X7J+7ckSTx/4yIQkbHzb/BKkAMO2XnX6f3xJEt20r/HEMhrcfZ+yMJTjdo/vil+uCn7vjnq+3oK5OYnVqDq56Zw3eWeG/Mj1j7u5GF9k3f7wBby0Pbyy0rwba/d9trV8a/O3l+3H3V5sx8omFmvdrlY+BVXKRS6Bc9MDkwU0e69epJbY+eoFRIelq5OP+P2Mpx7UVJAyhx4da5xNj5OMLcY5zuK6La04zVxd3LYd8dNZOlFfVorCsGpe+sQr3fO2/R8CylGwUVzRcZD+3IAWfrAmvd+VRH0MV7/5qc/1Quv98uxVvLkvF6OmLNDforLKCqtVyUSB/8jFEaufjF2paaUe1815a4ff5Q3n6DSEJtqeEHh9pvU+LEx+Zj4teXdmoeOOaz3HtAe0rhr64aG99Djv/5RW46FX/w9aqauuQ7va7mL/jGD5enVZ/EzCY4rFrdalSH0X0j1en4QtnW++hn3Zg6Z7jOOXJxVi5L7j5scxe1LBbLhrRq63Xx/dMn4xbz+gX4WiC9/LivbruT8/Pnxlz0bDHFuAPb/6O31Nz6h87XuxoX8wJoofy60tT8Z/vtgIAznp+GcY/HXjYmvv1cm5pFV5fmorvNBaMglFSWVPfKeHHTUdwOLcMpzy5GDNX+i+SuUSiWWRkT6SxAFKllAeklFUAvgbwBy/b3QXgBwD+b2MY5OTe7fDQxUPw/P/5H6N59am9cWK31o0ea9s8Hh1bNTMyPN0EOqn07NbnyXVxkF9a1aR3DxDeBYAed++MvgDZn12CTOfFU2pWSYCt0ah3VHUEe23M02GybZOyRC4CHMXqX+48w+82E07ohH9MPKHRY51bNUPbFvFNcpQZ+Wq8uzz00w7Dju26S11WVdOkd0+4XGmkoTefxlZcBJf2/m5jen2Pp60ZBQG3/3xtQ+E82F6U4VjsbDhpnR/LCj2QnCyTi9654RT8fMfpfrdJ6tQSX942rsnjLZvFYdLgLkaFFjGed8b15BoaX1Nbhz3H9J3M3pWLXL2JQjk//L2iVIfJcf/30/b6r48VBT7P3beP5LyAriKZZ68HX6xRzgZgoVz05W3j8MnNY/1u0zoxHgefvrjJ44nxsRh/QkejQrMNVz7amVmo635ducjV7gh06np72t9L0nUo9M/eFtyw+CvearheDicVBXvpme4c1heot6VLJNKkkUWkngDc+8lmOB+rJ4ToCeCPAN4xMA6/hBC47az+6NAyIeC2nVs3LhhZ5OanctnO1YjOeHYpTn9mqe5Fm4Ky4LpCA5G9S+Trxw21AKb8csl6n3tL5CLAUazWcge/ZUJs4wes9ztR6paPk3HOi78ZksNzgxyaYQZ2+fhYoEeSZXLR5GHdcHLvdgG3SwhxgYdo46vN8dyCFEx+ZaXmecWC8ePmI4E3CoHZarZCCNPFZAGWyUUTTuiEswd1Dridr/xv/j8LRgjuhDiQU4LZ2zIx5bVVPqfACMe/vtmq+z4Bcw6p1pqLIpWzjPz8G/nX31vYnm/ZKwAekFL6vTUthPibECJZCJGcnR25JVc9vXT1ybjv/EH1349N6qAsFiOVVtagqEL/uQIC9UAIleeEle6M/NthVALQe7e+9qfXcSzQeLNdLrr1zP64c9KA+u8vGtZNWSxGqqiuDXq+DHe+PpprDuRq3jaa+RuCo8fqUbpvb/5kZLtcdErf9o3aRVePMfdqMqGqqa2rvyGmp83OuXxy3SZ6NvvH2H9eCC/4UNpswRwz1Dah2X8nIdAtFwHmyUePX3ZS/U22UFfqMzspJY5r6L0XLNdICfcRE56f+0gsGx+MUNoo4Z7L/hdoCT9RBOytFeQhjMxdRs7cnwHAfcB8LwCe5c0xAL52Vo87AbhYCFEjpfzZfSMp5XsA3gOAMWPGKEvlnVs3w93nDkTfji0QHxuDCQM6+d1eCGv+4Tl1xmKUVdX6XAI+lIbUeS81zOlhrhSkPR69q7mhriZhlvfPQnd3bJeLEuNj8Z8LB2Nsvw5Izy/DdWMdS6THWOiXosWNH6zH+rQ8n7koFHd+uSnwRibX5Ndswb8zerJADyQX2+UiIQTuPncgJg3ughX7svGPs09wPq4qImM8OWc3Pl6dhq2PXYC2zeN12efrS/bVz0toJe53/0P9Pet9c8tun7cI0C0XAebJRzdNSMK5Q7rgszWH8J8LHfNH+it8dG7dzJDisJG+25iB/36/DT/9cwJG9WnvdZuckuBuvi3aeRyveCw+5E0kV8HTcko3ykUmuToySy6y+pxIGwAMFEL0E0IkALgGwC/uG0gp+0kpk6SUSQC+B/BPb8nJbP5wck9cPLx7/fc//nOC1+0SYmPw2/0TIxSVfsqqXGNXJf78wTosC7Dqhidvw8u0zAVkNPcTyiwnuTehVLLzS6t8rrak5Ue1+TWobXPRWYM64/pxfesvot++YbTPbdc+eG6kwtLN+rSGOUnu/24rvlof3OT23u7W+Rr/LqXOK0fa4KTy9zP4akwWV1T77MkabC4ycZoOlW1z0fBebXHHpAGIiXH81u4+d6DPba2yCIC7RbscS8cXV1TjpYUpeGlRcBPhepu74/N1DRPVG3nHX/fezQqSm/9c5P358qqmPVkjVaSywJLzts1Fvdq3wIMXD0G8c6jtGQN93/Bf+d9JkQpLN6652vZlleDLdYfx3+/DHyrWaE5Uj5PJzLlIRUPLby7y8VxVTR2yirX1HguUa8x0/WpYEUlKWQPgTjhm9N8N4Fsp5U4hxO1CiNuNOq4Ko/u0R6tm3jt19e1o/hVKfKmsqcPKfTm4/bONQb3u4gArbbj76PeDjdOTlxMwq6gCSVPnIGnqnIbNwu2OqMudNDWNBG9HHTV9ES5+Tfv7Hizh42sriKZc1LdjS1wwtKvX57q1TYxwNPr6bmMGHvxxe+AN3Vz2hvYFA55fsCfgNhXVtfW5yDXpvSmGUJnopBw+bSFGTNO+yqM/3t5ZE/2oQYumXNS2eTymXdpksSfHcy306ckTSe7n+WtLU/HaksB37d15m5/oeJH3HhBPzN4VsH1RVydx7ovLkTR1Tn2BKlK5yP9wjoiE4JN7e+7SN1Zh1PRFyo5vZtGUi+JjY7DoX2d5fS4xPtbr42bmfo7976ft+DY5/JXBdh31Prn/4t3HUVkTeCqSf3+7FUlT52DVPsdqaZFKA6HkIq3naKi5zNvr/vv9VoydsaTRYkkhX3uaoMnpYuRwNkgp5wKY6/GY1wnapJR/MTIWo7k+CwO7tMI+E/S60UONs59gZU1wK4RlBlhRx/3EefzXXY26hh/Iafre/fs7fSZk07PwpAc9qvs5JZX1d1t8TcypJewdR/RdkcFsoikXufTv1BIHDJis1U7ch0Et3p2F4T0blglesvt4k+3fXJZa/3VlTV39uedNRP/OB3GwUIvfnvnP8/vCsmpU1/n/W6HlyAdzSjGoa6vggrOQaMxFfTu2wKHc8FfRMYNIDZ38wm1lxLeX72/y/JfrD2N/tiO/78wsQu8OLXQ5rqbegtL712YgpWP1zaLympB7wLu/B/nO1TbN9nPqIZpykeu07dmuuddVoq3IqEyU6rFy7YWvrKj/+l/fbGmy/U+bM/DDJkcha9GuY357fuktpLkZI3guV9fW4VhhRX1Pr9o6iWDqlu6/Y62r1UaSPWcdU+it6xuGk8y8cYzCSMI3a0vDnbO7vtqs2349T+Bat0Gt3ooZ3opY7u04rQnhuFtXwsM6LAv5/sqDYe8jFO5JZcyTizF2xuIm2/ga2ubLtgztRSTPRrQFum1HpbvObZh4+7krRyiMJHwZ+Q3nq/tSz+HyvHOf5lZ0W7DzmOfmyPMyyXe4F5VVtcEV6b15dv6eoHsh6D1/wKjpCzHmyaa5qC6E5VP2HrfHjRhyOGtgw8pKl4zo7mdLa5k+e5dh+3bvGfCF27A3l91eeg7oUeDSsrqkv1zz/qqDmnou6M39R7/mvbU47eklXrcLdv6bL9cFN4SazC0x3p6Xva9qmM9Iq5LKxuev+59wb70nV+1rulCJHq0L19QD/vKNv+d2HS3y2mYzmnsumvbLTpz53LL661j3cLekFwS131A7VBhZNLPn2aRQV7chI2dpWJLSzCqqGy5ufjVgyUeXksqaoF8T6KRIOVaMVxfvQ4Hbym0FZQ1fh7Kcrmf77PkFKVpfGdRxAp3vns97K7L9R4cx0v6OSebXPL6ho+nVp/b2s6X5uReajWzUF4eUi/yfHel5ZXh9yT6vc6LoZc+xYhwN8i5VuMVficZ5wVet6MPfjS+2W2QESdSKEY5e2oD/uZLMzNvH+4NV+n229VnVx/s+XA/nllTizWWpXgtQQR3H7WtvdavZW73PO+fO1znr63G/+Uo2bhP6uim2wW2OPYpeN43vCwA+h9uanbdz4eXFwc3RZjSfE+c7n6iorsVby1PDPicD3aN6co5xhX5vPOdnW+kc3ueptLIGO454z8NW6vVo6HA2sjajGubB3izLLfF950iIpvs7nFtW3/1y7vbAjRkrWb0/B1XOopG/t3GnW48un40ynRKVWVZEIDKaew8/zws2b2dBVnEFznxuGQDg1SDnUDGCQOjnq2ee3XK4AIXl3ifQdhdKwd4fC7WvyIaMaxcFt+fGF0+eZ0XTfRVVVOMUZ09B7TfAfB3b/1kY6XN0z7Hi+sn8/b2NWT7moCKyIqOG1ga7V/eiVuBM5JhkesprK+uH4waMx8/PGfCme4ST0aHcsvo2jyPuxgG43quqIKeJMSsWkcgno4b++6sce3vKPdFIKZEVoDvyWc8vq/860+Cxz1nFFYgRAp1aNQv6tdtDmIfoupnr6r/29l7V1cn61XGI7EJFodJb48Nzclz3Meqem0sJTHh6af33NSEM6wqGq6DTo11zn9v4isDXqnf+ekfc8ME6n88BjoJb84TGg/+ZmciqjL4Y8dfe8nZsz3M2q6jCZ9tKAvjLh+tDD05DPO7Kq2pwKLc0pIVlvA0jDnTMZ+fv8btdbkklOgbRRrPK5Nikj8FdWyPleLHqMCzLcxqS3JJKxPg8iSSenrdbcwEJCH04G+Bod+07XoyBXVu7RaCNaxW8YCzefRyLnfNpeostPa8cg7u1bvK4TyZPRhzOppPbJ54AAGjuZcaslf+dhE2PnB/pkExr6Z6skF87c+UBjHtqia4TBmttHJZ5mWdo7IwlXucBUeWb5PQmj9Xq0Pj1l8Y4J5K5XHlKLwDAsJ5tmjy37n/nYsujzEUu4YyX/z01B6c9vQTzfVz0APoWjtwbJHuONW3wXvTqSkx4ZmmTx7Uoqgh+GF8gj8za0eSx8urg5krxlpvN3aQid+NPcEywOnlY03mQNj9yPtY/dG6kQzKtcIpUxworMPapJXjFz5CWTYcLQj+AB/eeSN7mGHpk1k6c/fxyv/vw9eN+YcBw5avfXRPU9lYaTkLadG3jmGrkLxOSmjz3wz8nYNUDkyIckXkF+/H37JF8ypOL8ffPfa/qneKl/aKHCi/ti1+3ZuL8l1fgQHbw8ywGulEWCvdJyn0JVDcy0zUXi0g6uWPSAKQ9M8Xraj29O7RAiwTrLSNpxsb6L865mWp1vDj7UKd5DaSUymfPP17kOL5798+ZKw7Ufx3uXAiOfdd/Ffa+SH8XntQNac9MQa/2TVfr6domEe1aJCiIKjxG3YwJJ48s2uUohrsmmvQMMZQ/9C/4GWbifmGTkR+4h6UrF4QjnIupQ7muLt0Nj7nPD5DjZ5gy2cPgbq2R9swUjD+hY5Pn2rdMQJfWiV5eZU5mviG80Vkg8lZcDtW0X3b6fM49L7zlZeU4T8FOZh3omP54+z25ej34+h1W1tTaZsUu8q51YjzSnpmCP49PavJcq2ZxXttLpI37uenqJBBKLx7Ae5vs8V99z2vkfuyftxzxuV1OieOGYUFZVdALD4XDV87x9/fk2+QMY4IxAItIBvjnxBMwZbh9ViAxE28TkYV71+jFRYEnpPtxU+CT+sPf03yuCBIMrT+Pt+2OFjS9cDzmdjH51Nzd9V/rsUIdmduMPw7DyF5tA29IQftBQ04I1hvLUgNuk6xhIsq1B3Ix7invuSjUu1jB5tk9R/1f0P6wMfD7Z+YLdwrOI5cMRY+2ieij01L0kWbm3ilGLHziWhnJG9d5qaXwkl1ciVO9rCBrJnuPNe6lkHwoP+x9mvnzEu3+PL4v2rWI99pD0hJM/NnScoPLH28rlvnrMe7q7V1eVYuPfk8LuP+Tn1gUdI/oUAkhUFoZ3rFe8zKXppnmoWURyQD/nXwi3rx+tM/nP7hpTASjCZ1Rk7bp6YKXV2BlqvfZ74HQVlty5xpC8tgs33flXBb6GdYSKn8J+Tm3eQBcXMPZ9PjNVVTXNlpanazn+nF9MevOM3w+/+L/jYxgNPY27dddjXr96a2q1jER43sajrHdx+pE7oJtiPjrOfTL1qZ3AMPNvUDDhVhtndR9gm6KrLMGdcbqB89Fopch/4CjyGQFFmgWYfHu47jrq83130spdY27xDn8dYlz7g9/9OiFFIxdmb57W+v1FqRm+R4aY6YLPPJuQJfW2PLoBejW1ntPyHvOHWiJ89wsIfqraZVW1WKi2zy1UuqbQ/NKHfnlSIH5rlVKK2t8tpvCyRPuNwIP5pTqOjInWCwiRUicc7Lja8f2wblDuiqOxrxCuXvzyM8Nc28EO/Y9ENcFm5awdJz9RNNWwc4d8MvWTKzZnxtwO9dS5Ld9mowznl3W5PkSz/lTTHxXhHwb2btd/fxJUUvnVtgMt55+k19Zqeu+7/t2C4DwTrdw7o7nlfm+Gzh3u+8CurfG0oq92Zi7I3DRfWemoxj2wsIUTHphef0QOV949996YmMEEuJicMsZ/VSH4pfKeSgCTR7rjXvvpCfn7PazZfD+9N5aAOY834LtRbQ9oxBf+lhYwJ3rZuXc7Udx3ku/+ZwAnKzLNY/kv84fhFgTV5GMPu2CzTeBtk/LbSjwLNmThbzSwKu6ajVz5UFnDLrtUjdHg5zeJD2vDG9q6I2+9oCjN/rBnFJMemE5XtYwmsYoLCJFSFxsDLZPuwBPXj5MdSi2Fuo4XF+ente0t0+wpJRIOea4O/bjpiO44X39J2vT4u6vNmua6PfamY4Gomv+kuS0PDzsVqhz7cPEf2MpgF1PXIjv/j5edRgUhLnbj6G4IvzGl+tO/bGiioDDTIxql9344Xps9dJt3dOjzh6grry++XABznnxt0bb1NXp28uCImvn4xdi22MXqA5DM6N6moRSKApu//ruL02HnoF73VbESpo6x++2RhTxauskLn1jlc/VKd25cpBrXslNh/Nx5nONFzLQ4z0hdX78x+nY9cSFqsPQzCx/94LNLXrMzerudz+jUbRyP3eTps5BVU1d2PsMRsqxYvz14w143s+8mJ5c816uT8vDZW+savTcwp3HIvL5YBEpglonxiOWy6/7ZZak6CmcBt4ri/fVV8sBYFVqDjINnMRRStlo9ZRgZeSXNxqHe9U73nt3VbhNTrf5cD5eWpgS1mpXFDktEuKQEMf0b7WhB6lZJWFdDC7efRy3fJJc/312caXmwpSWIXLeeOaiYC+0npy9CxudvQvu/WZLk+cl0KjBt+94MV5ZvDdgryUyh8T4WJ9D3MxE5Z1uM04tMGf70bDaRQVl1bjg5cYrFWkdlpEfYjvDc/fBzms39YdteH2po6fAu78dQHpe43Zcen5Zoxt1RwocbSnPJdDJnBLiYtAiIQ6Aea9F3Jml943qMN5clhowhkC/z4kvLG/0fUWNsfMmebaLbv98I8qCmAJg6g/b6js5rD+Yh20e7bONh/MbfT4Kyqrw5rJUTaNRghGn696IwmSWpOjukZ93oFZDYL56Qc3Z3nSCSi1LcS9zrnIQrJcX72vUfRQAVu8PrlL/UoDukdW1dY3mPPnjW6sBOCriX/+NPVyIjPDAD9vQoWXg1fXWHvDeUHBfHc1l+LSFAfeXmlWsqRejp7nbjzYZdrvPz3wi3rwfYPVMKSXmuQ2NO995YfrmslTsm3FxUMciIm2eX5CC284MPAQxNYiltSe+0HT4vKfiimpM87Naky+7jxbhji83NXosK8gVLL/ekO73eSmB//203fE1gDOfXYo66WhPpT0zJahjkVpj+3XA76n6XnBbhRmL1v6s3p/bpIjiSUr/E3R7usdtTjl/Qlk0oKSyBqOmL2r0WEFZFfLLtPc0D5SLIIH7v98KwFHMvuWT5PqbcXrmIt6KVuSnf05QHUJAKvKI0V26Q/HZ2kOoqI5s10YA+HRNWkiv+3LdoSaPXTdT3yF0jYpMbp+TrAhPoknhW/nfSapDCMioXORviIT5MhGw93hJ/Xh4f5aEWID2Zdme7JBeN8/LvEe3fZrsZcvQLdjpfXLf6loz/gbJny2Pnq86BJ+Mbg/5u3AzY7sIQKMe1r7813kho4Vnz55Qt/HG26pPLyzUdy6Ro4WNY1M43y2F6YObTlUdgk+ufGBYuyjIfFNTG/nrI0//+S5wnvno98D5ymVZSmhtHi289aQMpoCklXtb0d9iAOFgEUmRUX3ao5nJh5NYqxZtH3uPG3Oy66lRLym3vzcHsjmExGp6d2iBU/q2Vx0GmdBve41rSOklzW3YmsrJjyl87Vok4C8TklSH4ZXRdRyzForCpdePpaV4rlqx26IjNv11Ro3E+Fi8YNLVa8320SryXGzHpKLpnPT8UY36+2LuKobNrX/oPIzs1VZ1GL5ZrEujXbzhHHNv5ny351hx4I3IMj6/ZZypV0eyWvdqu3B1fzZzcaasyhoNWNLm4SlD8M4No1WH0QQXlDAHM+cispcrR/fE0n+frTqMJgrLHb1WmIu043tlDBaRFGrbPF7T/BbRpLTK2MnMiKip5gmx6NYmUXUYEedvYu06jkUgiri42Bj0at9CdRhNGL1ohN/hbIYemfTifrOfxS7rE0Kgf+dWqsNoYrmBQ63InozKRiwiKfbH0b0AAOcP7ao4kqZUFG6DWd7QruzarZ3MbfwJHQEAV4zqqTiSyJm15YjP537c7Pu5aGPmVexi3C6+zRwnadezXXMAwNVjeimOpCmjPmH+5us4WhjcBNCkRqM6IJtxtnL5yT1UhxAx3hbgIGsz6rKSq7MpdtnIHrhsZA/U1NahoLwaY54MfqZ3shertT2sFi95N6xnW6Q9MwVSSky96ESMfWqJ6pAMx4szbaxyV90qcZJ/7Vsm1K8g858LBkdFLvJcUZWIzMGVix6+ZCgmPLMUVTXqJ5ImMgP2RDKJuNgYdGrVTHUYZALhVozZkYnCIYRAlygc2kZE5mO6XMTObuRD4+FsZDedWjVDXAwTAIUnEtdoniNaOLE2RdzDP+9QHUJU4t10osZOf2ap6hDIpFg0p0gaO8P+vaIoNGy72Z+ZSkj/+ibwsvZEAOdEIooarouiUC+OuAoBERER2Y1Visac29KeuFIshSsSH6FIpR8WkYiIiMiS3BtkvG4jIjNgKiIiszCqbcQiEpHJuM71UKvVOSXGLkVMRKTFr1szVYdARDYSarvoqTm79Q2EolL7lvGqQyCLu/PLTRE/plFDbVlEMpm3rx+Nn+84XXUYpJCUEp+sTkNFda3qUCiKfXnrOHxy81jVYZBi32/MQHZxpeowfGLvI/ubfdcZeO3aUarDIMUW7DyGA9mlIb22uLJG52goGn1562mYfvkwJMTy8tlK9B5Btnp/DramF4T02q0ZhfoGo1Cc6gCosYuGd1cdAimWU1KFx37ZqToMinITBnRSHQKZwH++M/fknZyiwv6G9WyLYT3b4u6vNqsOhRT6+2cbVYegGYvb9tS7Qwv8+bS+mP7rLtWhkELXzVynOgS/PNMPh7MRERERERGZjPuFGmtIROYR7ecjV2eLMhcN64aHpwxBz3bNVYdCGnyx7pDqECKKk9lGj1vP6IebT++HW87opzoU0uDZ+XtUh6AMc5G9PXrJUFx4Uld8+/fxqkMhDe76anPUXrxxdTZ7e+3aURjRqy3m3XOm6lBIg/97Z41h8wJFMw5nM6m3bzgFgGNo0zu/7VccDQXy0E87VIdAZIiHLxkKAFi1LwcfrDqoOBoK5O3l0fX3gtdq0ePmM/rh5jP6oa6Ov3Qr+HVrJhLieK+a7GfysG6YPKyb6jCIvGrSLuJwNiIyozpexRGRCVTUcDECIlLDvSVk5sUIiKLR0cIK1SEow9XZohS735EZudeNvt6Qri4QihjmIjKjnJKGi7VpXJAgKjATWUdVTZ3qECKmoKy6/ut1B/MURkJEnn7cdER1CBFTXFHd6PvqWhaRolLb5vGqQyDy67Ul+1SHQBHQPD5WdQhETbgXsY1qKJG5cEE+MiNOPUFEZvDdxoyIHIdFJJO77cz+6NYmUXUYRBTlTunbHn8a01t1GEQU5WJiBF6/dpTqMIiI8PFfT1UdApESLCKZXHxsDJ65crjqMIgoygkhcPd5A1WHQUSES0f2UB0CEREmDu6iOgQiJVhEsoCJg7tg1QOTMKBLK9WhEFEU69muOVZPPQdTRnRXHQoRRbn1D52Le1nYJiLFNj9yPt66frTqMIgiikUki+jVvgXm3n0mdj8xWXUoRBTFerRrjlf/dDJ2Pn6h6lCIKIp1aZ2Ie84diG3TLlAdChFFsfYtE3Dx8O7Y+tgF6NK6mepwiCIiTnUApF1CHGt+RKReXGwM4mKZj4hILSEE2iRyARIiUq9t83iuHklRg1cBREREREREREQUEItIREREREREREQUEItIRERERERERGFo15zDayk6sIhkQc9fNUJ1CERE+OLWcapDICLCvHvOVB0CERE+uXms6hCIIoJFJAv6vzG9VYdARITTB3RSHQIREYZ0b4PeHZqrDoOIolyPds1xzam8TiP7YxGJiIiIiCwtVgjVIRARISaGuYjsz9AikhBishAiRQiRKoSY6uX564UQ25z/VgshRhoZDxFFJ+YiIjID5iLj8MKNSDvmIuMwFVE0MKyIJISIBfAmgIsADAVwrRBiqMdmBwGcLaUcAWA6gPeMioeIohNzkbFaNYtTHQKRJTAXGat3+xaqQyCyBOYiY3Vpnag6BCLDGdkTaSyAVCnlASllFYCvAfzBfQMp5WopZb7z27UAehkYj60suPcsvHrNyarDILIC5iIDLfjXWfj4r6eqDoPICpiLDPTaNaPwxnWjVIdBZAXMRQb6x8QTuAgS2Z6RRaSeANLdvs9wPubLLQDmGRiPrQzu1hqn9G2vOgwiK2AuMlDPds0xcXAX1WEQWQFzkYHatojHJSN6qA6DyAqYiwwUHxvDRZDI9owch+BtRKj0uqEQk+BIUGf4eP5vAP4GAH369NErPiKKDsxFRGQGzEVEZAa65SLnNsxHRFHGyJ5IGQDcy7C9AGR6biSEGAHgfQB/kFLmetuRlPI9KeUYKeWYzp07GxKsFfVsx+VsiTRgLiIiM2AuIiIz0C0XAcxHRNHIyCLSBgADhRD9hBAJAK4B8Iv7BkKIPgB+BPBnKeVeA2OxJcHlbIm0YC4iIjNgLiIiM2AuioDBXVurDoHIMIYVkaSUNQDuBLAAwG4A30opdwohbhdC3O7c7FEAHQG8JYTYIoRINioeu7rhNHYbJfKHuSgy7r9wsOoQiEyNuSgyXvnTyapDIDI15qLIeOIPJ6kOgcgwhq7NLKWcC2Cux2PvuH19K4BbjYzB7p68fDgAxyRuH/2epjYYIpNiLjLeHZMGIEYI7DhSiDnbj6oOh8iUmIuMd/monhAC+Oj3NGxJL1AdDpEpMRcZb1z/jvjhH+Px72+3Ii23THU4RLoycjgbRciTlw/Ho5cMVR0GEUW5f0w8AW9eP1p1GEQU5f5wck/8fMfpnDuSiJQ6pW8HLL9/Eu45d6DqUIh0xSKSTQghcO95TFBEpN77N45RHQIREb667TTVIRAR4V/nD1IdApGuWESykbvPYRGJiNQ7b2hXdGyZoDoMIopyfTq2wL958UZEJvDjPyeoDoFINywi2UhMjECbREOnuSIi0uTRSznElojUu/rU3oE3IiIy2Og+7VWHQKQbFpFsprpWqg6BiAjlVbWqQyAiYi4iIiLSGYtINvPAZC6zTUTqXXZyD9UhEBGhT4cWqkMgIiKyFRaRbKZrm0TVIRARoXl8rOoQiIgQEyNUh0BERGQrLCLZjGBbiYiIiIiIiIgMwCKSzYzo1U51CERERERERERkQywi2QyHsxGRGQghMKBLK9VhEBHhvCFdVYdARIRbz+inOgQiXbCIZDOxMQL7n7pYdRhERFh839ngdCREpNr7N43BNaf2Vh0GEUW5hy8Zivsv5CJIZH0sItlQbIzAnZMGqA6DiAif3zpOdQhERHjmyhGqQyAiwh28RiMbYBHJpvp1aqk6BCIidGrVTHUIRERERESkExaRbOqK0T3xwz/Gqw6DiKLcoK6tMeuO03HT+L6qQyGiKDf/3jPx/e1sGxGRWsv+MxEr/ztJdRhEIWMRyaaEEDilbwdcdUov1aEQUZQb2bsd/nRqH9VhEFGUO7FbG4xJ6qA6DCKKcv06tUTvDi1Uh0EUMhaRbG5k73aqQyAiQvOEWNUhEBERERFRmFhEsjkujEREZsBcRERERERkfSwi2dzFw7urDoGICD3bN1cdAhERAODMgZ1Uh0BEhL9MSFIdAlFIWESyuQ4tE/DslcNVh0FEUS4+NgZz7j5DdRhERPjgplNVh0BEhGmXnaQ6BKKQsIgUBa4e0xuXjeyhOgwiinIn9WiL564aoToMIopyCXExmH/vmarDICLC71PPUR0CUdBYRIoCQggM7tZadRhERBjYpZXqEIiIkNSxpeoQiIjQsx2H+5P1sIgUJc4e1Fl1CEREXNKWiEwhPpZNYCIiolDwL2iUGNazreoQiIjQqVUz1SEQESE2hmtGEpE5nNK3veoQiILCIlIU+fAvY1SHQDY3uk871SGQBfzwj/GqQyAiwuL7zuJQEiJS7u3rR+OMAVw1kqyDRaQocs6JXfHn0/qqDoNsLDE+VnUIZAGn9O2AD25iUZuI1BrQpTWW3z9RdRhEFOW6tEnE57eOUx0GkWYsIkWZycO6qQ6BiAgndm+jOgQiIsRxWBsREVFQWESKMqcP6ISTevDijYjU6tmuOe46Z4DqMIgoygkh8NM/J6gOg4gIqTMuUh0CkSYsIkWhZ68coToEIiIOryUiUxjSvQ1aJHA4NhGpFRcbg1GcX5QsgEWkKDSsZ1ukPTNFdRhkQ1KqjoCspEubRKQ9MwUcTUJEKiXGx2LXE5MxZXh31aEQUZT76Z+n46WrR6oOg8gvFpGiGFdIIiIzWPLviapDICLCU38crjoEIiJcOrKH6hCI/GIRKYqd0reD6hDIZiTYFYmC169TS9UhEBGhbYt4nDmQy2wTkVrxsTF46OIhqsMg8olFpCi35dHzVYdARITdT0xWHQIRET646VTVIRAR4dYz+3F+JDItFpGiXLsWCUh++DzVYZBNcE4kClXzhFjseuJC1WEQUZRLiIvBgacuxvCebVWHQkRRzLFy5On47+TBqkMhaoJFJEKnVs1w8OmLVYdBRFGuRUIcJ/0nIuViYgR+vesM1WEQEeGfEwfgX+cNUh0GUSMsIhEAR7V7zt1sMFF42BGJ9LDpEQ6zJSL19s24SHUIRES457yBqkMgaoRFJKp3Uo+2WPSvs3Tb3zd/O023fRFR9OjQMkHXOZLev3GMbvsiougRHxuja+9ILttNRKHSMxc9ftlJuu2LohOLSNTIwK6tcd/54XWZvHh4N/x65xkY17+jTlGRVQjVAZBtNE+IxYd/Ca/4c+3Y3vjgpjE4b2hXnaIiomi05N9nh/X60wd0xNNXDMcVo3vpFBERRaONYc5j27tDc9x3/iDcNCFJn4AoarGIRE3cfe7AoKvdzeIcH6XeHZrjretPwfBenJAyGnE4G+npnBO7Bp2LmsfH1n/99BUjcO4QFpCIKDwndG4VVi+Az28Zh2vH9mn02IQTmt5ou3Rkj5CPQUT217FVM6Q9MwWDu7YO6fVL7puIu89tPDTuPxc07Tzg7TEidywikU+vXnMypl06FF/dFnhY2pik9gCASYO7NHr8z6f1xVe3nYZbz+iHYT3bYPrlw3D2oM448BQn8iYibT69eSwenjIEM28c06hI5M05Qxw56ITOLRs9/q/zBuH1a0dh+uXD0LpZHD65eSxO7NYau5+YjBtO6+NtV0REjfx65xl4YPKJePlPI3HVKf57FU0a3Ln+ayEa+uk+fcVwPHrJUNx+9gkAgK//dhqSOrbAuv+di5vG9zUmcCKylfdvGoN7zxuIp68YjmeuGO53265tmtV/nRDXcOk/88Yx+MfEE3DR8O4AgM9uGYv+nVpi3j1n4s5zOAcT+RenOgAyrz+c3LP+602PnI/C8mr865st6N+5JX7cdASAo/HTr1NLtG0ej8d/3dlkKNz0y4cBAMa73XH782mORtKlI3vg162ZPo9/8OmL0e/Bubr9PADQqlkcSipr6r8/sVtr7DlWrOsxohq7IpEBzhrUGWcNclyQbXj4PBzJL8fHqw9iS3ohdh8tAgA8cslQXDqyOzq2bIZubRJx/bjGhSH3SSldOehs5z4D3dEzIhcRkfUM79W2vqf1lOE98JcJSUhOy8MTs3ehzvn379KRPfD4ZSehVbM4zFx5AOP6dWi0D/ceSa7eTcvvnwQAyCmp9Hrc9i3ikV9WjaX/PhvnvPib3j9WE+cN6YrFu48bfhwiCk3vDi1wr3PFNiklTuzeBkXl1bjxw/X12zSLi8H6h85DQmwMZm05gvYtExrt4/yhXXG+c7i/Kxct/c9Ev8c9fUBH/J6ai9evHYWXFu3FwZxSHX+qpl695mTc8/UWQ49BoWERiTTp0DIBHVom4Oc7TgcA3Hf+ILRIiEMHt4T09BUjgtrn01cMx2n9O+Chn3bgtWtHYeHOY7jznAHo1iYRpVW1EEJg8X1n47yX9GswvX7dKPz1ow3134/q045FJCILadUsDoO7ta7PN1lFFZAAurZJrN/mkUuGBrXP68b1RbO4WEyfvQu3TzwBOSWVOHtQZ4zt1wHHiyohhMDK/07Cmc8t0+3n+PtZ/fHpmkMor64FAPTp0AKH88p02z8RGSshLgbDerbFsJ5t8ZfT+yG/tApl1bXo3iYRMTGOnkd3TBoQ1D5P6tEWr/zpZLy+dB9O6dseJ3RuhXYt4nHl6F7Yn12K/p1bYf1D5+LWT5KxLaNQl5+jf6eW6Ny6GdYdzKt/bESvtiwiEVmEEAIn924HwFEMKqmsQW5JJXq1b4FYZy66ZmzwPa7fv3EM3l91ALV1Ejec1hdpOWW457yB2JlZiJN6tMVZgzrj1cX78OHvB3X7Wf52Vn+8t+JA/fcn9Wij275JXywiUUh6tW8R9j5aNYvD9eP64vpxjl4Bl7nNBdDOufsBXVph5o1j8PX6w1iyJyvsC61Jg7sg7Zkp+GLdITz00w5wKmh9SXZFogjr4lY8ClVsjMDVp/bG1af2bvJcv06OP5O9O7TAz3ecjneW78f8ncfC7sV4/4WD8eDFQ3AotxRnP7885P0QkTm0b5mA9jrs5/JRPXH5qJ5NHh/czdFjskvrRLx1/Wg8OXs35u88hhG92oZVUPrlrjPQqpkjzyVNnQMAkPxTTmRZrZrF1Z/T4ThvaFevC5Oc1MPRG7Nt83jcd8EgZOSXYdHu4xjeM7xctOL+SejTsQX+d/GQ+lxUx1xkWoYWkYQQkwG8CiAWwPtSymc8nhfO5y8GUAbgL1LKTUbGRNbj3t2yqqYOB3NK0aV1M7RvmYBdmUU4nFeKVs3iccMH69CpVbP67uCj+rTD5sMFeO7KEXjo5+0458SG+ZqGdHdUtsf164AdRwqx/Yj/pDeuXwd8dss4VNTU4paPN2BDWr7f7X+7fyI6tmqGYY8t0PQzTjihI1bvz9W0rVZtEuNQVFETeMMowFxEeji5dzu88+dTADi6j+/LKkG75vHo0iYRGfllWJ2ai7MHd8a4p5agdbM4FDuHzvbv3BIHsktxxeie2H20GJU1tYiLdcxL0K65ozfnhSd1RYuEOLy6ZF/AOPZMn4yK6lq8tiQ14B3AL24dh9F92mPIo/ObPNepVQJySqoaPda5dTNkF3sfUkPhYy4iPfRq36I+FwHAgewSJMTFoFf7FiiqqMa3G9Jxyxn9mgzDjYsRqKmT6NE2EWOSOuCXrZlNLjZPH9AREwZ0xMuLGx7r0TYRmYUVTeLY/cRklFfX4re9WfjXN1v9xvzk5cNw9ZjeGPTwvCbPndSjDXZmFjV5vF+nloYPl4lWzEWkh1bN4vDejQ0r6WYWlKO8uhYndG6Fmto6fLDqIG6akIQTH2naBnH59/mD8OKivejRrvFNwXYt4tGpVTMfr2ps27QLUFMrkVlQjkteX+V321vO6If/XTwEJ/yv6TQFQ7q3qZ8mwd3lJ/fAz1t8T8ESivhYgerahiqZ1YYRC2nQ7QYhRCyAvQDOB5ABYAOAa6WUu9y2uRjAXXAkqHEAXpVSjvO33zFjxsjk5GRDYiZrS88rQ+8OLXCkoByFZdUY2LUV4mN9zx2fU1KJTq2aoa5OoqKmFrklVUiMj8WPmzLw9Lw9GNuvA779+3jszCzE0O5tGk2MWVFdi9SsEvRs1xyjpi9qtF/3FVzGPbUYx4saLsjWPHgO2rdIqE+mM28cg0FdW6FX+xZYfzAP185cW79tz3bN0bZ5PHa5JbO4GIFzh3TBgp0NSebdP5+Cv3+2sVEMJ3Zrje9uH4+n5u7BV+sPAwC6tG6GNQ+eiyd+3Ym/nt4PrRPjcMqTi+FLr/bNkZFf7vW5v5/VH++6dTd1mXrRifWThQYihNgopQxvDXdtx2Euoog6UlCOrq2bIa+0CnllVejdvgVa+rkrmFdahbbN4xEjgNKqWpRV1aCyug5HCytw9btrAACpMy7CwZxS9OnYAs3iGiYXr6uT2JxegMHdWjcpWrvnokd+3oHP1h6q/37xfWfjhM4tMfapJcgursQdk07AdeP6omPLBKTlluJP765FYXk1AMeKd80TYpFX2rjgdOsZ/fD+qoYi1m1n9sPMlY2LWt3bJuL7f0zAvO1H8eSc3Y1ie2lhCs4d0hVJHVti5BMLA76v3lwyojtmbzva5PFLR/bA69eO0rQP5iKyq6yiCrRpHo/yqlpkl1Sic6tmTeZFcVdYVo3mCbFIiItBSWUNausksosr0at98/p2y7ZpFyC/tAodWiagdWJ8o9dvOpyPod3b4MJXVuBQbkOvcfdcNH/HUdz+eUMt4rkrR+DqU3vj1k+SsXj3cZya1B5vXj8aifGxKK+qxW2fNh66FxsjUOvRPeGxS4fi8V/rTyP0bNccRwoat19aN4vDN38fj6OF5bjlk4bzJe2ZKfj494NI6tQSY/t1wNBHtd388zS8Z1uvNyR7tW+OVQ+co2kfVs9FAPMReZdXWoX4WMd11LFCR17q6qcneUllDWKFQPOEWJRW1iA2RmB/dglO6tG2vpfSyv9OQlysQEJsDDp6FJt2ZRYhqVML3PXlZizZk1X/+PqHzkWX1o7jHi0sx/inl9Y/98dRPfHyn07G60v24cVFewEAWx49H3USSIyPwYM/bsesAIWkN68bjTu+9F9rbRYXg/dvGoNubRJx/ssr6h9Pe2YKZm05gto6iYuHd/dbePOnRUIsyqpqvT6ndZVRLbnIyCLSeADTpJQXOr9/EACklE+7bfMugOVSyq+c36cAmCilbNoidGJyIqPV1kks2nUcF57UtVHhyJfq2jrECIFXF+/FeUO7YkSvdvXP1dTWIflQPrq2SURpZQ2G9Wzrd1/lVbU4nFeGjPwynHNiFwgh8PnaQ3h7+X7MvftMtEqMQ4wAZm87iidm78LIXm3x/k2nImnqHFx4Ule8++em53tlTS3u/XoL/nfxEPTu0HgY4lfrD6N/p5b4fN1h3H52f/Rq3wLfbDiM0/p3RP/OrXDHF5vw295s3H3uQFwwtCu+2ZCOz9Yeqk9CrkQOOC4WV089R9N7BkS0scRcRJa1aNdxnD2oc6MVVXypqa0DAHy1IR092ibi3CEN3dCllEg+lI9ubRKRWVCOcf2bLi/urrZOYvfRIuSWVmFcvw5IjI/F6v05uG7mOmx65Hy0ahaH+FiBVak5mDFnN6pq67D03xMx/LEF6NgqoX6iYHdSSjz443ZcM7ZP/fwNLnO3H0W7FvH4Yt1h/GVCEgZ2aYVFu46jZ7vmGNK9Dd5fdQBvLtuPOycNwFmDOmNbRgGenLMbmx45Hx1aJqD/g3MadXtPnXFRfW+vQJiLiAL7PTUHJ/Vog3YtfBehXGrrJOqkxJLdWcgvq2o0mTgAbE0vQJc2zZByrBgTPVb19WZbRgEqquswqGsrtGuRgMO5ZTjr+WVY9cAkdGrVDInxsdieUYgZc3dh/cE8HHh6Cq56ezWSD+X7vGh6YUEKRvZuV9/T3WV1ag7Kq2ud7cBuGNazLbakF6BOSozq3Q7r0/Jw55ebcduZ/TDhhE6orZO49dNkzL7rDAzr2RZXv7MG69Ma5pfa/cRkNE/wv6qoi9VzEcB8RMbbfNhxXdWjXfOA20opUVVbh52ZRVi5N6fRIisAsO94MRLjY5FyrBjnnNilfj47X3YfLUJ5dS26OY9fWlmDkx5bgFl3nI4BXVqhZbM4pOeV4fFfd2Hx7uPYPu0CPDNvD75Yd9hnLvpkdRriY2NwnceCMDuOFGJfVjEOZJeif+eWGN+/Ew7nlSE9rwzj+ndASWUNJr+yEteO7YPT+nfA4G6tMfmVlXj92lG4dGQPPPHrrka91V3DBbVQXUS6CsBkKeWtzu//DGCclPJOt21mA3hGSrnK+f0SAA9IKX1mHyYnoqbq6iSEgOYCjl5KKmsQFyOQGGDZdW8i2FhiLiKKEFebItK5qKK6FnVSokVC8KP0mYuI7EdVLqqqqUNlTW2TXlpaWD0XAcxHRJ6klJASAQtUequtkyipqEHbFsbkIm236ULj7Z3yrFhp2QZCiL8JIZKFEMnZ2dm6BEdkJzExIuINJcAxFjqUAlKEMRcRRYgQanJRYnxsSAWkCGMuIooQVbkoIS4mpAJShOmWiwDmIyJ/hBARLyABjqG/oRSQtDKyiJQBwH2pm14APAcSatkGUsr3pJRjpJRjOnfurHugRGRrzEVEZAbMRURkBrrlIoD5iCgaGVlE2gBgoBCinxAiAcA1AH7x2OYXADcKh9MAFAYaa0tEFCTmIiIyA+YiIjID5iIiCothfb+llDVCiDsBLIBj+cgPpZQ7hRC3O59/B8BcOGb9T4Vj+ci/GhUPEUUn5iIiMgPmIiIyA+YiIgqXoRMISCnnwpGE3B97x+1rCeAOI2MgImIuIiIzYC4iIjNgLiKicBg5nI2IiIiIiIiIiGyCRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgpIOFZwtA4hRDaAQxo37wQgx8BwQsGYtDFbTGaLB7B+TH2llJ2NDMZIzEWGYEyBmS0ewPoxMRepxZi0YUyBmS0eIIpyEWD5fGS2eADGpJXZYjJbPIDOuchyRaRgCCGSpZRjVMfhjjFpY7aYzBYPwJisxIzvC2PSxmwxmS0egDFZiRnfF8akDWMKzGzxAOaMySzM9t6YLR6AMWlltpjMFg+gf0wczkZERERERERERAGxiERERERERERERAHZvYj0nuoAvGBM2pgtJrPFAzAmKzHj+8KYtDFbTGaLB2BMVmLG94UxacOYAjNbPIA5YzILs703ZosHYExamS0ms8UD6ByTredEIiIiIiIiIiIifdi9JxIREREREREREenAtkUkIcRkIUSKECJVCDHVwON8KITIEkLscHusgxBikRBin/P/9m7PPeiMKUUIcaHb46cIIbY7n3tNCCHCiKm3EGKZEGK3EGKnEOIe1XEJIRKFEOuFEFudMT2uOibnvmKFEJuFELNNEk+ac19bhBDJJompnRDieyHEHudnarzqmKyEuYi5SGNczEWBY2IuCkOkcpHzWKbKR8xFQcXFXBQ4JuaiMDAXMRdpjMtUuci5P1PlI6W5SEppu38AYgHsB9AfQAKArQCGGnSsswCMBrDD7bHnAEx1fj0VwLPOr4c6Y2kGoJ8zxljnc+sBjAcgAMwDcFEYMXUHMNr5dWsAe53HVhaX8/WtnF/HA1gH4DQTvFf3AfgSwGyT/O7SAHTyeEx1TJ8AuNX5dQKAdqpjsso/MBcxF2mPi7kocEzMRaG/dxHLRc7jmSofgbkomLiYiwLHxFwU+nvHXMRcpDUuU+Ui5/7SYKJ8BIW5SHkyMeKf801Y4Pb9gwAeNPB4SWicnFIAdHd+3R1Airc4ACxwxtodwB63x68F8K6O8c0CcL5Z4gLQAsAmAONUxgSgF4AlAM5BQ4JS+h7Be3JS+R61AXAQzvnTzBCTlf6BucgzPuYi73EwFwWOh7kovM96RHOR8xhJMGk+AnORrziYiwLHw1wU3meduahxbMxF3uMwXS5y7iMNJslHUJyL7DqcrSeAdLfvM5yPRUpXKeVRAHD+3yVAXD2dX3s+HjYhRBKAUXBUlZXG5eyWuAVAFoBFUkrVMb0C4L8A6tweU/27kwAWCiE2CiH+ZoKY+gPIBvCRs0vp+0KIlopjshLmIifmIr9eAXNRIMxF4VGdiwCT/K6Yi/x6BcxFgTAXhYe5yIm5yK9XYL5cBJgrHynNRXYtInkbxycjHkVTvuIyJF4hRCsAPwC4V0pZpDouKWWtlPJkOKrLY4UQw1TFJIS4BECWlHKj1pcYGY+b06WUowFcBOAOIcRZimOKg6Mb8NtSylEASuHoGqkyJisx68/NXMRcFAhzkb2Y+eeO2O+KucjPzpmLtGIuCo+Zf27mIuaiQMyUj5TmIrsWkTIA9Hb7vheAzAge/7gQojsAOP/PChBXhvNrz8dDJoSIhyM5fSGl/NEscQGAlLIAwHIAkxXGdDqAy4QQaQC+BnCOEOJzhfEAAKSUmc7/swD8BGCs4pgyAGQ470gAwPdwJCxTfJYsgLmIuSgQ5iJtmIvCozoXAYp/V8xFATEXacNcFB7mIuaiQEyZiwDT5SOluciuRaQNAAYKIfoJIRIAXAPglwge/xcANzm/vgmO8a6ux68RQjQTQvQDMBDAemdXs2IhxGnO2dBvdHtN0Jz7+ADAbinlS2aISwjRWQjRzvl1cwDnAdijKiYp5YNSyl5SyiQ4Ph9LpZQ3qIoHAIQQLYUQrV1fA7gAwA6VMUkpjwFIF0IMdj50LoBdKmOyGOYi5iK/mIu0YS4Km+pcBKj9TDMXBcBcpA1zUdiYi5iL/DJjLgLMl4+U5yKpw6RgZvwH4GI4ZrzfD+AhA4/zFYCjAKrhqOTdAqAjHJOB7XP+38Ft+4ecMaXAbeZzAGPg+CDuB/AGPCbJCjKmM+DohrYNwBbnv4tVxgVgBIDNzph2AHjU+bjS98q5v4lomLRN5XvUH45Z87cC2On63Kp+jwCcDCDZ+bv7GUB71TFZ6R+Yi5iLtMc2EcxF/uI6GcxF4Xy+IpKLnMcyVT4Cc1GwsU0Ec5G/uE4Gc1E4ny/mIuYirbFNhAlykXNfpstHUJiLhPOFREREREREREREPtl1OBsREREREREREemIRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgqIRSQiIiIiIiIiIgqIRSQKihCioxBii/PfMSHEEefXJUKItww87kQhxASj9k9E1sJcRERmwFxERGbAXESRFKc6ALIWKWUugJMBQAgxDUCJlPKFCBx6IoASAKsjcCwiMjnmIiIyA+YiIjID5iKKJPZEIl04q9CznV9PE0J8IoRYKIRIE0JcIYR4TgixXQgxXwgR79zuFCHEb0KIjUKIBUKI7s7H7xZC7BJCbBNCfC2ESAJwO4B/OSvqZwohOgshfhBCbHD+O93t2J8JIZYKIfYJIW5zPt5dCLHC+fodQogzlbxRRGQo5iIiMgPmIiIyA+YiMgJ7IpFRTgAwCcBQAGsAXCml/K8Q4icAU4QQcwC8DuAPUspsIcSfAMwAcDOAqQD6SSkrhRDtpJQFQoh34FZRF0J8CeBlKeUqIUQfAAsADHEeewSA0wC0BLDZeaxrASyQUs4QQsQCaBGZt4GIFGMuIiIzYC4iIjNgLqKwsYhERpknpawWQmwHEAtgvvPx7QCSAAwGMAzAIiEEnNscdW6zDcAXQoifAfzsY//nARjqfC0AtBFCtHZ+PUtKWQ6gXAixDMBYABsAfOissP8spdyiw89IRObHXEREZsBcRERmwFxEYWMRiYxSCQBSyjohRLWUUjofr4PjcycA7JRSjvfy2ikAzgJwGYBHhBAnedkmBsB4ZyKq50xY0mNbKaVcIYQ4y7nvz4QQz0spPw3xZyMi62AuIiIzYC4iIjNgLqKwcU4kUiUFQGchxHgAEELECyFOEkLEAOgtpVwG4L8A2gFoBaAYQGu31y8EcKfrGyHEyW7P/UEIkSiE6AjHZG8bhBB9AWRJKWcC+ADAaKN+MCKyFOYiIjID5iIiMgPmIgqIRSRSQkpZBeAqAM8KIbYC2AJgAhxdJj93drHcDMeY2gIAvwL4o3PStTMB3A1gjHBM7LYLjkndXNYDmANgLYDpUspMOBLVFiHEZgBXAnjV+J+SiMyOuYiIzIC5iIjMgLmItBANPdiIrE9EdklLIiKvmIuIyAyYi4jIDJiL7IU9kYiIiIiIiIiIKCD2RCIiIiIiIiIiooDYE4mIiIiIiIiIiAJiEYmIiIiIiIiIiAJiEYmIiIiIiIiIiAJiEYmIiIiIiIiIiAJiEYmIiIiIiIiIiAJiEYmIiIiIiIiIiAL6fxk4P2yvOmO5AAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 1440x360 with 4 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(timesteps)\n",
    "ax.set_ylabel(\"Time steps\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "plt.show()\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(5, 5))\n",
    "ax.plot(rewards)\n",
    "ax.set_ylabel(\"Rewards\")\n",
    "ax.set_xlabel(\"Episode\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots(nrows=1, ncols=4, figsize=(20, 5))\n",
    "for i, key in enumerate(losses):\n",
    "    flattened_loss = [loss for episode_loss in losses[\"total\"] for loss in episode_loss]\n",
    "    ax[i].plot(flattened_loss, label=key)\n",
    "    ax[i].set_ylabel(\"Losses\")\n",
    "    ax[i].set_xlabel(\"Timesteps\")\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f414fa5d864e6fd9d435e1cda9914dfbcdafa27af1ac4ac359054dbf9afa5fd9"
  },
  "kernelspec": {
   "display_name": "Python 3.9.10 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.10"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
